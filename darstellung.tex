% Dies ist Teil der Vorlesung Physik auf dem Computer, SS 2012,
% Axel Arnold, Universitaet Stuttgart.
% 
% Dieses Werk ist unter einer Creative Commons-Lizenz vom Typ
% Namensnennung-Weitergabe unter gleichen Bedingungen 3.0 Deutschland
% zugänglich. Um eine Kopie dieser Lizenz einzusehen, konsultieren Sie
% http://creativecommons.org/licenses/by-sa/3.0/de/ oder wenden Sie sich
% schriftlich an Creative Commons, 444 Castro Street, Suite 900, Mountain
% View, California, 94041, USA.

% !TEX root = padc.tex

\chapter{Darstellung von Funktionen}

Auch moderne Prozessoren beherrschen nur die Grundrechenarten. Wie
kann man also auf einem Computer kompliziertere Funktionen berechnen,
wie \zb die Sinusfunktion?

Beispielsweise könnte man die Funktionen als Vektor von
Funktionswerten speichern. Für die graphische Darstellung reicht das
aus, aber um Funktionen mit wenigstens sechs Stellen Genauigkeit im
Computer bereitzustellen, wären Millionen von Stützstellen nötig.

Daher müssen bessere Darstellungen für Funktionen genutzt werden.  Um
beliebige Funktionen auf dem Computer berechnen zu können, führt man
diese meist auf (stückweise definierte) Polynome zurück, die nur mit
Hilfe der Grundrechenarten berechnet werden können. Dies ist selbst
dann der Fall, wenn ein Prozessor gewisse Funktionen scheinbar in
Hardware implementiert hat; tatsächlich führt dieser intern die
notwendigen elementaren Operationen durch.

\section{\keyword{Horner-Schema}}
\label{sec:horner}

Die naive Auswertung eines Polynoms $\sum_{i=0}^{n} c_ix^{i}$ mit
$n+1$ Termen bzw.\ vom Grad $n$ benötigt $n$ Additionen und $2n$
Multiplikationen sowie einen Zwischenspeicher für die Potenzen $x^i$
des Arguments $x$. Besser ist die Auswertung des Polynoms nach dem
Horner-Schema:
\begin{equation}
  \label{eq:horner}
  \sum_{i=0}^{n} c_ix^{i} = c_0 + x(c_1 + x(c_2 + x(\ldots (c_{n-1} + x c_{n})))).
\end{equation}
Wird dieser Ausdruck von rechts nach links ausgewertet, so muss das
Ergebnis in jedem Schritt nur mit $x$ multipliziert und der nächste
Koeffizient addiert werden, was nur $n$ Multiplikationen und
Additionen benötigt. Auch muss kein Zwischenwert gespeichert werden,
was Prozessorregister spart. In Python sieht die Auswertung des
Hornerschemas so aus:%
\lstinputlisting[firstline=10]{horner.py}%
\sloppypar Die Polynomauswertung stellt NumPy als
\scipy{numpy.polyval(c, x)} zur Verfügung. \argd{c} bezeichnet die
Koeffizienten des Polynoms und \argd{x} das Argument, für das das
Polynom ausgewertet werden soll. Achtung! Anders als das obige
Beispiel folgt \lstinline!polyval! \emph{nicht} der mathematischen
Schreibweise und verarbeitet die Koeffizienten in der umgekehrten
Reihenfolge, berechnet also $\sum_{i=0}^{n} c_ix^{n-i}$.

Eine weitere Anwendung des Hornerschemas ist die Polynomdivision durch
lineare Polynome der Form $x-x_0$, die zum Beispiel wichtig für die
iterative Bestimmung von Nullstellen ist. Es gilt nämlich
\begin{equation}
  \label{eq:polynomdiv}
  P(x) = \sum_{i=0}^{n} c_ix^{i} = 
  \left(\sum_{i=0}^{n-1} d_{i+1}x^{i}\right)(x-x_0) + d_0,
\end{equation}
wobei $d_i = c_i + x_0(c_{i+1} + x_0(\ldots (c_{n-1} + x_o c_n)))$ die
Zwischenterme des Hornerschemas bei Auswertung an der Stelle $x_0$
sind. $d_0$ ist dabei der Divisionsrest; ist $P(x)$ durch $x-x_0$
teilbar, so ist $d_0=0$. 

Dies zeigt man durch Induktion: für $P(x) = c_1 x + c_0$ ist offenbar
$P(x) = c_1(x-x_0) + c_0 + x_0 c_1 = d_1(x-x_0) + d_0$. Für Grad $n$ ist
also
\begin{equation}
    P(x) = x \left(\sum_{i=0}^{n-1} c_{i+1}x^{i}\right) + c_0
    = x\left(\sum_{i=0}^{n-2} d'_{i+1}x^{i}(x-x_0) + d'_0\right) +
    c_0
\end{equation}
wobei sich die $d'_i = c_{i+1} + x_0(c_{i+2} + x_0(\ldots (c_{n-1} +
x_o c_n))) = d_{i+1}$ bei der Polynomdivision von $\sum_{i=0}^{n-1}
c_{i+1}x^{i}$ durch $x-x_0$ nach Voraussetzung ergeben. Daher ist
\begin{equation}
  P(x) = \sum_{i=0}^{n-2} d_{i+2}x^{i+1}(x-x_0) + d_1 x + c_0
  = \left(\sum_{i=0}^{n-1} d_{i+1}x^{i}\right)(x-x_0) +
  \underbrace{d_1 x_0 + c_0}_{d_0},
\end{equation}
was zu zeigen war.

\section{Taylorreihen}
\index{Taylorreihe}

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{plots/sinus}
  \caption{Näherung der Sinusfunktion durch die abgeschnittene
    Taylorreihe. Als schwarze durchgezogene Linie ist die tatsächliche
    Sinusfunktion dargestellt, blau gepunktet ist die Näherung erster
    Ordnung um Null, $x$, grün durchgezogen ist die kubische Näherung
    $x - x^3/6$, und rot gestrichelt $x-x^3/6 + x^5/120$. Die Kurven
    nutzen die Symmetrie der Sinuskurve, sind also an $\pm\pi/2$
    gespiegelt.}
  \label{fig:sinus}
\end{figure}

Nachdem wir nun wissen, wie Polynome effizient ausgewertet werden
können, stellt sich die Frage, wie man ein gutes Näherungspolynom für
eine Funktion bekommt. Dazu gibt es viele verschiedene Ansätze, deren
Vor- und Nachteile im Folgenden kurz besprochen werden. Der älteste
Ansatz, der auch in der Analytik weiten Einsatz findet, ist die
Taylorentwicklung.  Ist eine Funktion $f$ um einen Punkt $x_o$
hinreichend gut differenzierbar, lässt sie sich als bekannterweise
lokal als Taylorreihe darstellen:
\begin{equation}
  f(x) = \sum_{i=0}^\infty \frac{f^{(i)}(x_0)}{i!} (x-x_0)^i,
  \label{eq:taylor}
\end{equation}
wobei $f^{(i)}(x)$ die $i$-te Ableitung von $f$ an der Stelle $x$
bezeichnet.  Falls die Ableitungen existieren und $x-x_0$ klein genug
ist, so konvergiert diese Darstellung schnell, und einige Terme
genügen, um zufriedenstellende Genauigkeit zu erreichen. Lokal um den
Entwicklungspunkt $x_0$ is eine abgeschnittene Taylorreihe also eine
gute polynomielle Näherung. Leider gibt es für die meisten Funktionen
einen Konvergenzradius, außerhalb dessen die Reihe nicht einmal
konvergiert. Daher eignen sich Taylorreihen nur, wenn kleine 
Umgebungen genähert werden sollen. Um weitere Intervalle so nähern zu
können, ist es aber natürlich möglich, mehrere Entwicklungspunkte zu
benutzen, so dass deren Umgebungen, in denen die Konvergenz ausreicht,
den gesamten geforderten Bereich abdecken.

Um  zum  Beispiel die  oben  angeführte  Sinusfunktion  mit 7  Stellen
Genauigkeit im Intervall $[0:\pi/2]$  auszuwerten, genügen die ersten 7
Terme der Taylorreihe. Mit Hilfe der Symmetrien der Funktion lässt sie
sich damit bereits für alle Argumente auswerten. Da
\begin{equation*}
  \sin'(x) = \cos(x) \quad\text{und} \quad \cos'(x) = -\sin(x),
\end{equation*}
ergibt sich die bekannte Reihe
\begin{equation}
  \sin(x) = \sum_{i=0}^\infty \frac{\sin^{(i)}(0)}{i!} x^i =
  \sum_{i=0}^\infty \frac{(-1)^i}{(2i+1)!} x^{2i+1}.
\end{equation}
Wie gut diese Darstellung mit entsprechender Rückfaltung funktioniert,
zeigt Abbildung~\ref{fig:sinus}.  Für viele andere komplexe Funktionen
ist es ebenfalls möglich, Taylorreihen analytisch oder numerisch zu
bestimmen, die dann zur Auswertung auf dem Computer genutzt werden
können.

\section{Polynom- oder Lagrangeinterpolation}
\index{Interpolation}
\index{Interpolation>Polynom-}
\index{Interpolation>Lagrange-}

Wie besprochen ist eine abgeschnittene Taylorreihe nur im
Entwicklungspunkt exakt (dann allerdings auch die Ableitungen),
innerhalb des Konvergenzradius nur eine Annäherung, und außerhalb des
Konvergenzradius sogar divergent.  Oft möchte man aber eher für einen
größeren Wertebereich eine gute (oder wenn möglich exakte) Darstellung
der Funktion haben.

Eine Möglichkeit dazu bietet die Polynom- oder
Lagrangeinterpolation. Dazu legt man eine Anzahl von Punkten im
gewünschten Wertebereich fest (die sogenannten \emph{Stützstellen}).
Wie sich zeigt, gibt es dann genau ein Polynom, dass die Funktion an
diesen Punkten exakt interpoliert. Genauer: seien Punkte $(x_i, y_i)$,
$i=0(1)n-1$ gegeben mit $x_i$ paarweise verschieden. Dann gibt es
genau ein Polynom $P(x)=\sum_{k=0}^{n-1} a_kx^{k}$ vom Grad $n-1$, so
dass $P(x_i) = y_i$, da die Gleichung
\begin{equation}
  \begin{split}
    y_1 &= P(x_0) = a_0 + a_1 x_0 + \cdots + a_{n-1}x_0^{n-1}\\
    \vdots\\
    y_n &= P(x_{n-1}) = a_0 + a_1 x_{n-1} + \cdots + a_{n-1}x_{n-1}^{n-1}
  \end{split}
  \label{eq:interpol}
\end{equation}
genau eine Lösung hat.  In SciPy liefert die Funktion
\scipy{scipy.interpolate.lagrange(x, y)} das interpolierende Polynom
durch die Stützstellen (\argd{x[i]}, \argd{y[i]}).

Leider ist aber nicht gewährleistet, dass mit steigender Anzahl von
Punkten die Funktion auch zwischen den Stützstellen immer besser
angenähert wird. Tatsächlich hat Runge ein einfaches Beispiel
angegeben, nämlich die Rungefunktion $1/(1+x^2)$, für die die Näherung
mit steigender Anzahl an äquidistanten Punkten immer schlechter
wird. Abbildung~\ref{fig:runge} zeigt Polynome wachsenden Grades, die
diese Funktion interpolieren. Bei äquidistanten Stützstellen wird der
Interpolationsfehler zum Rand des Intervalls hin sehr groß.

Bei der etwas allgemeineren Hermite-Interpolation können an den
Stützstellen neben den Funktionswerten auch Ableitungen vorgegeben
werden. Das eindeutige interpolierende Polynom hat dann einen Grad,
der der Gesamtanzahl an vorgegebenen Funktionswerten und Ableitungen
entspricht. Ist zum Beispiel nur eine Stützstelle $x_0$ gegeben und
neben dem Funktionswert $n$ Ableitungen, so entspricht das
Hermite-Polynom genau den ersten $n+1$ Termen der Taylorreihe.

Das interpolierende Polynom kann nicht nur zur Interpolation verwendet
werden, also der Bestimmung an Punkten zwischen den Stützstellen,
sondern --- mit Vorsicht --- auch zur
Extrapolation\index{Extrapolation}, also um Werte außerhalb des
Bereichs der Stützstellen zu bestimmen. Da bei der
Hermite-Interpolation auch die Ableitungen insbesondere am Rand
kontrolliert werden können, ist diese hier tendenziell
vorteilhafter. Bei einem Taylorpolynom wird sogar fast immer
extrapoliert, da es ja nur eine Stützstelle gibt. Extrapolation ist
immer dann wichtig, wenn die Auswertung der Zielfunktion an der
Zielstelle numerisch zu teuer oder unmöglich wird. So werden wir
später das Romberg-Integrationsverfahren kennenlernen, bei dem zu
Schrittweite 0 extrapoliert wird, also \enquote{unendlich vielen}
Stützstellen. Bei Computersimulationen nutzt man dies insbesondere in
der Nähe von kritischen Punkten.

Die Koeffizienten $a_i$ können im Prinzip als Lösung von
Gleichung~\eqref{eq:interpol} mit geeigneten Lösern für lineare
Gleichungssysteme gefunden werden, was im Allgemeinen allerdings recht
langsam ist. Daher lernen wir nun effizientere Darstellungen des
interpolierenden Polynoms, die unter geeigneten Umständen die
Berechnung und Auswertung des interpolierenden Polynoms mit linearen
Aufwand ermöglichen.


\subsection{\keyword{Lagrangepolynome}}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/runge_lagrange}
  \caption{Lagrange-Interpolation der Rungefunktion $1/(1+x^2)$
    (schwarze Linie). Im linken Graph sind die Stützstellen
    äquidistant gewählt (markierte Punkte), die farbigen Linien sind
    die interpolierenden Polynome durch 3 (rot), 5 (lila), 7 (blau)
    und 11 (grün) Stützstellen. Mit steigender Ordnung wird die
    Interpolation am Rand immer schlechter, das Polynom 10. Ordnung
    erreicht Werte bis nahe an zwei. Im rechten Graph sind
    für die gleichen Ordnungen Chebyshev-Stützstellen gewählt worden,
    die den Interpolationsfehler minimieren.}
  \label{fig:runge}
\end{figure}

Die einfachste Darstellung benutzt die \emph{Lagrangepolynome}, die
wie folgt definiert sind:
\begin{equation}
  \label{eq:lagrange}
  L_i(x) = \prod_{k\neq i} \frac{x-x_k}{x_i-x_k}.
\end{equation}
Die Polynominterpolation wird daher auch Lagrange-Interpolation
genannt.  Wie man leicht sieht, gilt $L_i(x_k) = \delta_{ik}$, so dass
das Polynom\index{interpolierendes Polynom>Lagrangedarstellung}
\begin{equation}
  P(x) = \sum_{i=0}^{n-1} y_i\,L_i(x)
\end{equation}
das eindeutige interpolierende Polynom durch $(x_i, y_i)$ ist. Die
Auswertung der Lagrangedarstellung erfordert $\O(n)$ Operationen für
jedes der $L_i(x)$, also ingesamt $\O(n^2)$ Operationen. Das ist zwar
besser also der Aufwand für die Lösung des Gleichungssystems
\eqref{eq:interpol}, dafür muss er aber bei jeder Auswertung erbracht
werden.  Die Lagrangedarstellung ist daher nur dann effizient, wenn
die Stützstellen und der Punkt, an dem ausgewertet werden soll, fest
sind, da dann lediglich die äußere Summe neu bestimmt werden muss.

\subsection{Baryzentrische Darstellung}
\index{interpolierendes Polynom>baryzentrische Darstellung}%
Eine alternative Darstellung ist die \emph{baryzentrische Darstellung}
\begin{equation}
P(x) = \sum_{i=0}^{n-1} y_i\, \mu_i(x) \; \Big/ \; \sum_{i=0}^{n-1}
\mu_i(x)\quad\text{mit}~
\mu_i(x) := \frac{1}{x-x_i}\omega_i \quad\text{und}~
\omega_i = \prod_{k\neq i}\frac{1}{x_i-x_k}
\end{equation}
Sind die Stützstellen $x_i$ fest, so können die Werte $\omega_i$
vorausberechnet werden. Dann kann das Polynom an beliebigen Stellen
mit Aufwand $\O(n)$ ausgewertet werden. Allerdings kann der Ausdruck
an den Stützstellen selber nicht ausgewertet werden.

\subsection{\keyword{Neville-Aitken-Schema}}

Die effizienteste Möglichkeit,das interpolierende Polynom auszuwerten,
ohne es zu berechnen, ist das rekursive Neville-Schema. Das ist
nützlich, wenn nur wenige Auswertungen nötig sind, wie zum Beispiel
beim Romberg-Integrationsverfahren, bei dem zur Schrittweite 0
extrapoliert wird.

Wir definieren $P_{i,k}$ als das interpolierende Polynom der Ordnung
$k-1$ durch die Stützstellen $x_j, j=i(1)i+k-1$.  Gesucht ist der Wert
$P(x)=P_{0,n}(x)$ des interpolierenden Polynoms an der Stelle $x$.
Dann ist
\begin{align}
  P_{i,1}(x)&=y_i \quad\text{für}~ i=0(1)n-1
  \intertext{und}
  \label{eq:divdiff}
  P_{i,k}(x)&= \frac{P_{i,k-1}(x)(x_{i+k-1} - x) + P_{i+1,k-1}(x)(x -
    x_i)}{x_{i+k-1} - x_i} \quad\text{für}~ k=2(1)n, i=0(1)n-k,
\end{align}
da ja an den inneren Stützstellen $x_l$, $l=i+1(1)i+k-2$,
$P_{i,k-1}(x_l)=P_{i+1,k-1}(x_l) = y_l$ gilt, und per Konstruktion
$P_{i,k}(x_i)=y_i$ und $P_{i,k}(x_{i+k-1})=y_{i+k-1}$. Durch
sukzessives Berechnen zunächst der $P_{i,2}(x)$, dann der
$P_{i,3}(x)$, \usw lässt sich das interpolierende Polynom bequem an
einer fixen Stelle auswerten. Als (Neville-)Schema sieht das so aus:
\begin{center}
  \begin{tikzpicture}[on grid=true, node distance=1.5em and 10em]
    \node (u01) {$y_0$};
    \node[below=of u01] (u11) {$y_1$};
    \node[below=of u11] (u21) {$y_2$};
    \node[below=of u21] (u31) {$y_3$};
    \node[below=of u31] (u41) {$\vdots$};
  
    \node[right=of u11] (u02) {$P_{0,2}(x)$};
    \node[below=of u02] (u12) {$P_{1,2}(x)$};
    \node[below=of u12] (u22) {$P_{2,2}(x)$};
    \node[below=of u22] (u32) {$\vdots$};
  
    \node[right=of u12] (u03) {$P_{0,3}(x)$};
    \node[below=of u03] (u13) {$P_{1,3}(x)$};
    \node[below=of u13] (u23) {$\vdots$};

    \node[right=of u13] (u04) {$P_{0,3}(x)$};
    \node[below=of u04] (u14) {$\vdots$};

    \draw[->] (u01) -- (u02);
    \draw[->] (u11) -- (u02);
    \draw[->] (u11) -- (u12);
    \draw[->] (u21) -- (u12);
    \draw[->] (u21) -- (u22);
    \draw[->] (u31) -- (u22);

    \draw[->] (u02) -- (u03);
    \draw[->] (u12) -- (u03);
    \draw[->] (u12) -- (u13);
    \draw[->] (u22) -- (u13);

    \draw[->] (u03) -- (u04);
    \draw[->] (u13) -- (u04);
  \end{tikzpicture}
\end{center}
wobei die Pfeilpaare dividierte Differenzen gemäß \eqref{eq:divdiff} bedeuten.

\subsection{Newtonsche Darstellung}
\index{interpolierendes Polynom>Newtonsche Darstellung}

Wir betrachten nun die Polynome $P_{0,k}$ des Nevilleschemas. Es gilt
offenbar
\begin{equation}
  P_{0,k}(x) - P_{0,k-1}(x) = \gamma_k(x-x_0)\cdots(x-x_{k-2}),
\end{equation}
da die beiden Polynome in den Stützstellen $x_0,\ldots,x_{k-2}$
übereinstimmen und die Differenz ein Polynom vom Grad $k-1$ ist, also
höchstens $k-1$ Nullstellen hat. Weiter ist $\gamma_k$ der führende
Koeffizient des Polynoms $P_{0,k}(t)$, da $P_{0,k-1}(t)$ ja einen
niedrigeren Grad hat. Daraus ergibt sich die folgende \emph{Newtonsche
  Darstellung} des interpolierenden Polynoms:
\begin{equation}
  \begin{split}
    P_{0,n}(x) &= y_0 + \sum_{k=2}^{n} P_{0,k}(x) - P_{0,k-1}(x)\\
    &= y_0 + \gamma_2(x-x_0) + \gamma_3(x-x_0)(x-x_1) + \cdots
    + \gamma_n(x-x_0)\cdots(x-x_{n-2})\\
    &= y_0 + (x-x_0)\biggl(\gamma_2 + (x-x_1)\Bigl(\gamma_3 + \cdots
    \bigl(\gamma_{n-1} + (x-x_{n-2})\gamma_n\bigr)\cdots\Bigr)\biggr).
  \end{split}
\end{equation}
Die letztere Umformung zeigt, dass sich die Newtonsche Darstellung
effizient mit einem leicht abgewandelten Hornerschema auswerten lässt:
\begin{lstlisting}
def horner(x, xsupport, gamma):
    r = 0
    for k in range(len(xsupport)-1, -1, -1):
        r = r*(x-xsupport[k]) + gamma[k];
    return r
\end{lstlisting}

Die Koeffizienten $\gamma_i$, $i=2(1)n$ lassen sich dabei bequem mit
dem Nevilleschema bestimmen. $\gamma_k$ ist ja der höchste Koeffizient
von $P_{0,k}$ ist, der sich leicht aus \eqref{eq:divdiff} berechnen
lässt. Wenn $\gamma_{i,k}$ den führenden Koeffizienten des Polynoms
$P_{i,k}$ bezeichnet, so erhalten wir das Nevilleschema
\begin{align}
  \gamma_{i,1} &= y_i \quad\text{für}~ i=0(1)n-1\quad\text{und}\\
  \gamma_{i,k}&= \frac{\gamma_{i+1,k-1} - \gamma_{i,k-1}}{x_{i+k-1} -
    x_i}
  \quad\text{für}~ k=2(1)n, i=0(1)n-k.
\end{align}
Da letztlich nur die $\gamma_{0,k}$ interessant sind, also die obere
Diagonale des Nevilleschemas, benötigt man für die Berechnung nur
einen Vektor
\begin{equation}
  \gamma' = \left(\gamma_{0,1},\gamma_{0,2},\ldots,\gamma_{0,k-1},
    \gamma_{0,k},\gamma_{1,k},\ldots,\gamma_{n-k,k}\right),
\end{equation}
der wie folgt berechnet wird:
\begin{lstlisting}
def neville(x, y):
    n = len(x)
    gamma = y.copy()
    for k in range(1, n):
        for i in range(n-k-1, -1, -1):
            gamma[i+k] = (gamma[i+k] - gamma[i+k-1])/(x[i+k] - x[i])
    return gamma
\end{lstlisting}
Die Schleife über $i$ zählt abwärts, um benötigte Werte nicht zu
überschreiben.

\subsection{\keyword{Chebyshev-Stützstellen}}

Bis jetzt haben wir wenig zur Wahl der Stützstellen gesagt. Oft liegt
es auch nahe, äquidistante Stützstellen zu verwenden wie im
Fadenpendel-Beispiel. Man kann allerdings zeigen, dass die
Chebyshev-Stützstellen den Fehler der Polynominterpolation minimieren.
Diese sind definiert als die Nullstellen der Polynome (!)
\begin{equation}
  \label{eq:chebyshev}
  T_n(\cos\phi) = \cos(n\phi),
\end{equation}
die offensichtlich zwischen -1 und 1 liegen und daher geeignet
skaliert werden müssen für die Interpolation in einem allgemeinen
Intervall. Die Chebyshev-Polynome $T_n$, $n\ge 0$, bilden eine
orthogonale Basis der Funktionen über $[-1,1]$ bezüglich des mit
$1/\sqrt{1-x^2}$ gewichteten Skalarprodukts. Daher kann jede genügend
glatte Funktion auf $[-1,1]$ als eine Reihe
\begin{equation}
  f(x) = \sum_{n=0}^\infty a_n T_n(x)
\end{equation}
dargestellt werden, die sogenannte Chebyshev-Reihe (siehe auch \zb
\textcite{abramowitz70a}).

 Explizit sind diese Nullstellen gegeben durch
\begin{equation}
  x_{k,n} = \cos\left(\frac{2k+1}{2n}\pi\right),\quad k=0(1)n-1.
\end{equation}
Wird die Rungefunktion mit Chebyshevstützstellen interpoliert, so
konvergiert das interpolierende Polynom, im Gegensatz zu äquidistanten
Stützstellen. Sollen die Stützstellen statt $[-1,1]$ das Intervall
$[a,b]$ abdecken, skaliert man einfach entsprechend um:
\begin{equation}
  x_{k,n} = \frac{a+b}{2} + \frac{a-b}{2}\cos\left(\frac{2k+1}{2n}\pi\right),\quad k=0(1)n-1.
\end{equation}


\section{Splines}
\index{Spline}
\index{Interpolation>Spline-}

Wie wir gesehen haben, kann unter ungünstigen Umständen die Güte der
Polynominterpolation mit steigender Anzahl an Stützstellen sinken, vor
allem, wenn diese äquidistant verteilt sind. Oft ist das aber nicht zu
vermeiden, zum Beispiel, wenn die Daten in einem Experiment regelmäßig
gemessen werden. Das Problem ist, das die Koeffizienten des Polynoms
global gelten, sich glatte Funktionen aber nur lokal wie ein Polynom
verhalten (Taylorentwicklung!). Daher ist es günstiger, statt der
gesamten Funktion nur kleine Abschnitte durch Polynome zu nähern.

Der einfachste Fall einer solchen Näherung ist die \emph{lineare
  Interpolation}\index{Interpolation>lineare}, bei der die
Stützstellen durch Geraden, also Polynome vom Grad 1, verbunden
werden. Sind die Stützstellen $(x_i, y_i)$, $i=0(1)n-1$ gegeben, so ist
der lineare interpolierende Spline
\begin{equation}
  P_1(x) = \frac{(x_{i+1} - x)y_i + (x - x_i)y_{i+1}}
  {x_{i+1}-x_i}
  \quad\text{für}~ x_i \le x < x_{i+1}.
\end{equation}

Diese Funktionen sind aber an den Stützstellen im Allgemeinen nicht
differenzierbar. Soll die Interpolierende differenzierbar sein, müssen
Polynome höherer Ordnung genutzt werden. Solche stückweise definierten
Polynome heißen \emph{Splines} --- das englische Wort Splines
bezeichnete dünne Latten, die vor dem Computerzeitalter benutzt
wurden, um glatte, gebogene Oberflächen vor allem für Schiffsrümpfe zu
entwickeln.

Der wichtigste Spline ist der \emph{kubische}
Spline\index{Spline>kubisch}, der aus Polynomen dritten Grades
zusammengesetzt und zweifach stetig differenzierbar ist. Seine
allgemeine Form ist
\begin{equation}
  P_3(x) = y_i + m_i(x-x_i) + \frac{1}{2}M_i(x-x_i)^2 + \frac{1}{6}\alpha_i(x-x_i)^3
  \quad\text{für}~ x_i \le x < x_{i+1}.
\end{equation}
auf $n-1$ Intervallen $[x_i, x_{i+1}]$, $i=0(1)n-2$.  Da die zwei
rechten und linken zweiten Ableitungen an den Stützstellen
übereinstimmen müssen, gilt
\begin{equation}
  \label{eq:splinealphas}
  \alpha_i = \frac{M_{i+1} - M_i}{x_{i+1} - x_i} \quad\text{für}~ i = 0(1)n-2,
\end{equation}
wobei $M_{n-1}$ die Ableitung des Splines an seinem rechten Ende
bezeichnet.  Aus der Gleichheit der Funktionswerte an den Stützstellen
ergibt sich
\begin{equation}
  \label{eq:splinems}
  m_i = \frac{y_{i+1} - y_i}{x_{i+1}-x_i} -
  \frac{1}{6}(x_{i+1}-x_i)(2M_i + M_{i+1}) \quad\text{für}~ i = 0(1)n-2.
\end{equation}
Aus der Gleichheit der ersten Ableitungen ergibt sich schließlich ein
Gleichungssystem mit $n-3$ Gleichungen für die $M_i$, nämlich
\begin{equation}
  \begin{pmatrix}
    \mu_1  & 2         & \lambda_1 \\
           &           & \ddots    & \ddots    & \ddots \\
           &           &           & \mu_{n-3}  & 2         & \lambda_{n-3}
  \end{pmatrix}
  \begin{pmatrix}
    M_0\\
    \vdots\\
    M_{n-2}
  \end{pmatrix} \,=\,
  \begin{pmatrix}
    6S_1\\
    \vdots\\
    6S_{n-3}
  \end{pmatrix},
\end{equation}
mit
\begin{equation}
  \lambda_i = \frac{x_{i+1}-x_i}{x_{i+1} - x_{i-1}},\quad
  \mu_i = \frac{x_i - x_{i-1}}{x_{i+1} - x_{i-1}}\quad\text{und}\quad
  S_i = \frac{\frac{y_{i+1}-y_i}{x_{i+1} -
      x_i} - \frac{y_i-y_{i-1}}{x_i - x_{i-1}}}{x_{i+1}-x_{i-1}}.
\end{equation}
Sind die $M_i$ bestimmt, so ergeben sich die $m_i$ und $\alpha_i$ aus
\eqref{eq:splinealphas} bzw.  \eqref{eq:splinems}. Allerdings gibt es
fehlen dazu noch zwei Gleichungen, nämlich die Randbedingungen.

Soll der Spline am Rand festgelegte 2. Ableitungen $P_3''(x_0) = M_0$
und $P_3''(x_{n-1}) = M_{n-1}$ haben, so hat das Gleichungssystem die
Form
\begin{equation}
  \label{eq:spline1}
  \begin{pmatrix}
    1      & 0         &           &           &           &         0 \\
    \mu_1  & 2         & \lambda_1 \\
           &           & \ddots    & \ddots    & \ddots \\
           &           &           & \mu_{n-2}  & 2         & \lambda_{n-2} \\
    0      &           &           &           & 0         & 1         \\
  \end{pmatrix}
  \begin{pmatrix}
    M_0\\
    M_1\\
    \vdots\\
    M_{n-2}\\
    M_{n-1}
  \end{pmatrix} \,=\,
  \begin{pmatrix}
    P_3''(x_0)\\
    6S_1\\
    \vdots\\
    6S_{n-2} \\
    P_3''(x_{n-1})
  \end{pmatrix}.
\end{equation}
$M_{n-1}$ wird zur Interpolation nicht benötigt, da es nur die zweite
Ableitung am rechten Rand darstellt. Diese Form des Splines wird auch
als \emph{natürlicher} Spline bezeichnet.\index{Spline>natürlich}

Auch periodische Funktionen können kubisch interpoliert werden, wobei
dann die zusätzlichen Bedingungen durch die stetige
Differenzierbarkeit über die periodische Grenze hinweg gegeben
sind. Dadurch besteht der Spline aus $n$ statt $n-1$ Abschnitten,
wobei der letzte Abschnitt den periodischen Zirkelschluss formt. Die
Gleichungen für $\alpha_i$ und $m_i$ gelten dann unverändert für
$i=0(1)n-1$, wobei $x_n = x_0$, $y_n = y_0$ und $M_n = M_0$
indentifiziert werden. Das Gleichungssystem nimmt dadurch die Form
\begin{equation}
  \label{eq:spline2}
  \begin{pmatrix}
    2      & \lambda_0 &           &           &           &      \mu_0 \\
    \mu_1  & 2         & \lambda_1 &           & 0\\
           &           & \ddots    & \ddots    & \ddots \\
           & 0          &           & \mu_{n-2}  & 2         & \lambda_{n-2} \\
    \lambda_{n-1} &           &           &           & \mu_{n-1}  & 2         \\
  \end{pmatrix}
  \begin{pmatrix}
    M_0\\
    M_1\\
    \vdots\\
    M_{n-2}\\
    M_{n-1}
  \end{pmatrix} \,=\,
  \begin{pmatrix}
    6S_1\\
    6S_2\\
    \vdots\\
    6S_{n-2} \\
    6S_{n-1}
  \end{pmatrix},
\end{equation}
wobei in $\lambda_i$, $\mu_i$ und $S_i$ wieder $x_{n} = x_0$
und $x_{-1} = x_{n-1}$ identifiziert werden.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/splines}
  \caption{Links: Spline-Interpolation der Rungefunktion
    (durchgezogene schwarze Linie). Die gestrichelte blaue Linie ist
    die lineare Spline-Interpolierende mit 7 Stützstellen, die anderen
    Kurven sind kubische Splines mit 7 (grün gepunktet) und 11
    Stützstellen (rot gestrichelt). Mit 11 Stützstellen ist der Spline
    von der Rungefunktion praktisch nicht mehr zu
    unterscheiden. Rechts: erste (rot gepunktet) und zweite Ableitung
    (blau) des interpolierenden Splines elfter Ordnung, sowie die
    zweite Ableitung der eigentlichen Rungefunktion (dünn
    schwarz). Hier zeigen sich deutliche Unterschiede, da der Spline
    ja ein Polygonzug niedriger Ordnung ist.}
  \label{fig:spline}
\end{figure}

Abbildung~\ref{fig:spline} zeigt die Spline-Interpolation der
Rungefunktion. Anders als die Lagrangeinterpolation hat die
Splineinterpolation keine Probleme mit der Rungefunktion. Die
Spline-Interpolierende elften Grades lässt sich kaum von eigentlichen
Funktion unterscheiden. Allerdings gilt das nur für die Funktion
selber. Die Ableitung und vor allem zweite Ableitung unterscheiden
sich deutlich, da die letztere ja ein Linienzug ist.

Die Gleichungssysteme \eqref{eq:spline1} und \eqref{eq:spline2} sind
sehr gut konditioniert und mit einem einfachen Gleichungslöser zu
behandeln.  Zum Beispiel ist die Gauß-Elimination ist für die hier
auftretenden einfachen Bandstrukturen sehr effizient. In SciPy gibt es
selbstverständlich bereits eine fertige Routine für die
Spline-Interpolation, nämlich \scipy{scipy.interpolate.interp1d(x, y,
  kind)}. (\argd{x}, \argd{y}) sind dabei die Stützstellen, und
\argd{kind} eine Zeichenkette, die den Typ des Splines
bestimmt. Mögliche Werte sind zum Beispiel "`linear"' und "`cubic"'
für lineare bzw.\ kubische interpolierende Splines. Allerdings erlaubt
diese Form nicht, die Randbedingung zu wählen. SciPy implementiert
eine weitere Methode, bei der die Quadratnorm der zweiten Ableitung
minimiert wird, es sucht also den \enquote{glattesten} Spline.

\section{Fourierreihen}

Bis jetzt waren unsere Näherungsfunktionen auf Polynomen basierend, da
diese einerseits vom Computer verarbeitet werden können und
andererseits aufgrund der Taylorentwicklung glatte Funktionen meist
gut approximieren. Für periodische Funktionen sind Polynome aber an
sich erst einmal wenig geeignet, da sie selbst nicht periodisch
sind. Splines können zwar auch periodisch gemacht werden, aber
trotzdem sind trigonometrische Funktionen besser geeignet, um
periodische Funktionen darzustellen. Fourierreihen und
-transformationen stellen Funktionen als trigonometrische Reihen dar,
die meist gut konvergieren und darüber hinaus einige nützliche
Eigenschaften haben.

Es gibt zwei Hauptanwendungen der Fourierdarstellung: die Analyse und
Aufbereitung periodischer Signale und die Lösung von
Differentialgleichungen.  Bei periodischen Signalen dient die
Fourierdarstellung zur Analyse des \emph{Spektrums} des Signals.
Diese gibt nützliche Informationen über die charakteristischen
Zeitskalen von Strukturen im Signal, zum Beispiel die Tonhöhe und die
Obertonreihe eines Instruments. In dieser Frequenzdarstellung lassen
sich auch gezielt einzelne Frequenzen dämpfen, was Rauschen
unterdrücken kann und im ursprünglichen Funktionsraum teure Faltungen
erfordert.  Bei Differentialgleichungen nutzt man aus, dass die
Ableitung im Frequenzraum eine algebraische Operation ist, und die
Differentialgleichung somit in eine gewöhnliche algebraische (und oft
sogar lineare) übergeht.

\subsection{Komplexe Fourierreihen}
\index{Fourierreihen>komplexe}
\index{Fouriertransformation>komplexe}
Wir betrachten eine periodische Funktion $f(t)$ mit $f(t+T) = f(t)$
für alle $t\in\RR$, \dh $f$ hat Periode $T$. Dann ist die
eindeutige Fourierdarstellung von $f$ gegeben durch
\begin{equation}
  \label{eq:fourier}
  f(t) = \sum_{n\in\ZZ} \hat{f}_n e^{i n\omega t}
\end{equation}
mit $\omega=2\pi/T$. Die Koeffizienten $\hat{f}_n$ lassen sich
berechnen als
\begin{equation*}
  \label{eq:fouriercoeff}
  \hat{f}_n = \frac{1}{T}\int_0^T f(t)e^{-i n\omega t}\, dt
\end{equation*}
und sind im Allgemeinen komplex, auch wenn $f$ reellwertig ist. Die
Beiträge $\hat{f}_{\pm n}$ haben dieselbe Frequenz $\pm n/T$,
unterscheiden sich aber in ihrer Phase. Die \emph{Leistung} zu dieser
Frequenz ist $\lvert\hat{f}_{n}\hat{f}_{-n}\rvert$.

\eqref{eq:fourier} lässt sich auch so lesen, dass
\begin{equation}
  e^{-i n\omega t} = \cos(n \omega t) + i \sin(n \omega t)
\end{equation}
eine orthonormale Basis bezüglich des Skalarprodukts
\begin{equation}
  \label{eq:l2scalar}
  (f, g) = \frac{1}{T}\int_0^T f(t)\overline{g(t)}\,dt
\end{equation}
bilden. Ähnlich wie ein Vektor im $\RR^n$ wird die Funktion $f$ also
durch die Fouriertransformation in ihre Schwingungskomponenten
zerlegt.  Insbesondere sind die Fourierkoeffizienten linear in der
Funktion, \dh
\begin{equation}
  \widehat{(f+\lambda g)}_n = \hat{f}_n +\lambda \hat{g}_n.
\end{equation}

Die Voraussetzungen für die Konvergenz der Fourierreihe sind sehr
schwach - solange die Funktion wenigstens quadratintegrabel ist,
konvergiert die Fourierreihe fast überall, \dh
\begin{equation}
  \left\lVert f(t) - \sum_{n=-N}^N \hat{f}_n e^{i n\omega t} \right\rVert \xrightarrow{N\to\infty} 0.
\end{equation}
Daneben ist die Transformation $f\to\hat{f}$ eine \emph{Isometrie},
genauer gilt das \emph{\keyword{Parsevaltheorem}}
\begin{equation}
  \sum_{n\in\ZZ} \lvert\hat{f}_n\rvert^2 =
  \frac{1}{\omega}\int_0^T \lvert f(t)\rvert^2\,dt.
\end{equation}
Das Parsevaltheorem besagt auch, dass die Restbeiträge von großen $n$
immer kleiner werden, so dass also eine abgeschnittene Fourierreihe
eine Approximation an die gesuchte Funktion darstellt. Anders als
abgeschnittene Taylorreihen, die nur in einer schmalen Umgebung um den
Aufpunkt exakt sind, konvergiert die Fourierreihe
gleichmäßig. Allerdings muss die abgeschnittene Fourierreihe im
allgemeinen keinen einzigen Punkt mit der Zielfunktion gemeinsam
haben, anders als Taylorreihen oder Splines.

Weiter gilt:
\begin{itemize}
\item Die Fourierreihe über einem Intervall $[0,T)$ kann aus der
  Fourierreihe für das Intervall $[0,2\pi)$ durch Streckung mit
  $\omega$ berechnet werden:
  \begin{equation}
    \widehat{f(t)}_n = \frac{1}{T}\int_0^T f(t)e^{-i n\omega t}\, dt
    = \frac{1}{2\pi}\int_0^{2\pi} f(t'/\omega)e^{-i n t'}\, dt',
  \end{equation}
\item Es gilt
  \begin{equation}
    \widehat{f(t + t_0)}_{n} = e^{i n \omega t_0} \widehat{f(t)}_{n}
  \end{equation}
  die Phase kann also nach Belieben verschoben werden. Die Leistung
  $\hat{f}_{n}\hat{f}_{-n}$ bleibt dabei natürlich erhalten.
\item Für die komplexe Konjugation gilt stets
  $\widehat{\overline{f}}_n = \overline{\hat{f}_n}$, da die
  Fouriertransformation ja linear ist.
\item Ist Funktion $f$ symmetrisch, also $f(t) = f(-t) = f(T-t)$, so ist
  $\hat{f}_{-n} = \hat{f}_n$, also $\hat{f}$ symmetrisch.
\item Ist Funktion $f$ ungerade, also $f(t) = -f(-t) = -f(T-t)$, so ist
  $\hat{f}_{-n} = -\hat{f}_n$, also $\hat{f}$ ungerade.
\item Ist Funktion $f$ reellwertig, also $f(t) = \overline{f(t)}$, so
  ist $\hat{f}_{-n} = \overline{\hat{f}_n}$. Allerdings sind die
  Fourierkoeffizienten im Allgemeinen komplex!
\item 
  Ist die komplexwertige Funktion $f(t)=g(t) + ih(t)$ mit $g$, $h$
  reellwertig, gilt also
  \begin{equation}
    \hat{f}_{n}  + \overline{\hat{f}}_{n} = 2\widehat{g}_{n}
    \quad\text{und}\;
    \hat{f}_{n}  - \overline{\hat{f}}_{n} = 2i\widehat{h}_{n}.
  \end{equation}
  Dies bedeutet, dass sich die Fourierreihen zweier reellwertiger
  Funktionen zusammen berechnen und anschließend wieder trennen
  lassen. Da die Berechnung der Fourierkoeffizienten sowieso komplex
  erfolgen muss, erspart dies bei numerischer Auswertung eine
  Transformation.
\item Die Ableitung der Fourierreihe ist sehr einfach:
  \begin{equation}
    \frac{d}{dt}  f(t) = \sum_{n\in\ZZ} \hat{f}_n i n\omega e^{i n\omega
      t} = \sum_{n\in\ZZ} \widehat{\left(\frac{df}{dt}\right)}_n e^{i n\omega
      t}\quad\implies\quad \widehat{\left(\frac{df}{dt}\right)}_n= i n\omega\hat{f}_n.
  \end{equation}
\end{itemize}

\subsection{Reelle Fourierreihen}
\index{Fourierreihen>reelle}
\index{Fouriertransformation>reelle}

Da die Fourieranalyse besonders zur Analyse und Bearbeitung von
Messdaten genutzt wird, sind die Fourierreihen reellwertiger
Funktionen besonders wichtig. Ist die Funktion $f$ rellwertig, so ist
\begin{multline}
  \hat{f}_{n}e^{in \omega t} + \hat{f}_{-n}e^{-in \omega t} =
  \hat{f}_{n}e^{in \omega t} + \overline{\hat{f}_{-n}e^{in \omega t}}
  = 2\Re(\hat{f}_{n}e^{in \omega t})\\
  = 2\Re(\hat{f}_{n})\cos(n \omega t) - 2\Im(\hat{f}_{n})\sin(n \omega t).
\end{multline}
Daraus folgt, dass sich die Fourierreihe auch komplett reellwertig
schreiben lässt:
\begin{equation}
  f(t) = \frac{a_0}{2} + \sum_{n=1}^\infty a_n \cos(n\omega t) + b_n
  \sin(n\omega t)
\end{equation}
mit
\begin{align}
  a_n &= 2\Re(\hat{f}_{n}) = \frac{2}{T}\int_0^T f(t)\cos(n\omega t)\,
  dt
  \intertext{und}
  b_n &= -2\Im(\hat{f}_n) = \frac{2}{T}\int_0^T f(t)\sin(n\omega t)\,
  dt.
\end{align}
Für symmetrische Funktionen ist offenbar $b_n=0$, für ungerade
Funktionen $a_n=0$.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/fourier}
  \caption{Abgeschnittene Fourierreihen der Rechteckfunktion (links)
    und eines Dreieckpulses (rechts). Die Funktionen sind jeweils als
    schwarze durchgezogene Linien eingezeichnet, die Näherungen mit
    einem Term blau gestrichelt, mit zwei Termen rot gepunktet, und
    mit 20 Termen grün durchgezogen. Für den Dreieckpuls ist letztere
    Näherung nicht mehr von der Funktion zu unterscheiden, während der
    Rechteckpuls noch deutliche Artefakte an den Unstetigkeiten zeigt.}
  \label{fig:fourier}
\end{figure}
Einige reelle Fourierreihen sind zum Beispiel:
\begin{itemize}
\item Konstante $f(t) = f_0$:
  \begin{equation}
    a_0 = 2 f_0,\quad a_n,\,b_n= 0\quad\text{sonst}
  \end{equation}
\item Rechteckfunktion
  \begin{equation}
    f(t) = \begin{cases}
      1 &\text{für}\; 0 \le t < \frac{T}{2} \\
      -1  &\text{für}\; \frac{T}{2} \le t < T
    \end{cases}\\
    = \quad\frac{4}{\pi}\sum_{n=1}^\infty
    \frac{1}{2n-1}\sin\left((2n-1)\omega t\right)
  \end{equation}
\item kurzer Rechteckpuls. Wir betrachten nun die auf konstanten
  Flächeninhalt normierte Funktion
  \begin{equation}
    f_S(t) = \begin{cases}
      1/S &\text{für}\; 0 \le t < S \\
      0  &\text{für}\; S \le t < T
    \end{cases},
  \end{equation}
  deren Fourierreihe
  \begin{equation}
    f_s(t) =
    \quad \frac{1}{T} +
    \quad\frac{2}{T}\sum_{n=1}^\infty
    \frac{\sin(n\omega S)}{n\omega S}\cos(n\omega t) +
    \frac{1-\cos(n\omega S)}{n\omega S}\sin(n\omega t)
  \end{equation}
  ist. Je kleiner $S$ wird und damit der Träger von $f_S$, desto
  langsamer konvergiert ihre Fourierreihe, da die Funktion $\sin(x)/x$
  immer dichter an der Null ausgewertet wird. Für jede feste Frequenz
  $n$ gilt schließlich
  \begin{equation}
    \widehat{\left(f_S\right)}_n \xrightarrow{S\to 0} \frac{1}{T} = \hat\delta_n
    \quad\text{für alle}\;n\in\ZZ
  \end{equation}
  bzw. $a_n\to 2/T$ und $b_n\to 0$. Die $\delta$-Funktion, die ja der
  formale Grenzwert der $f_S$ ist, und den kleinstmöglichen
  Träger hat, hat also in gewisser Weise die am schlechtesten
  (tatsächlich gar nicht!) konvergierende Fourierreihe.
\item Dreiecksfunktion
  \begin{equation}
    f(t) = \begin{cases}
      t      &\text{für}\; 0 \le t < \frac{T}{2} \\
      T - t  &\text{für}\; \frac{T}{2} \le t < T
    \end{cases}\quad=\quad\frac{\pi}{2} -
    \frac{4}{\pi}\sum_{n=1}^\infty
    \frac{1}{(2n-1)^2}\cos\left((2n-1)\omega t\right)
  \end{equation}
\end{itemize}
Genau wie die komplexe Fourierreihe lässt sich natürlich auch die
reelle Fourierreihe abschneiden, um Näherungen für Funktionen zu
bekommen, vergleiche Abbildung~\ref{fig:fourier}. Es fällt auf, das
die Fourierreihe besonders schlecht dort konvergieren, wo die Funktion
nicht differenzierbar ist bzw.\ einen Sprung aufweist.

\subsection{Diskrete Fouriertransformation}
\index{Fouriertransformation>diskrete}
\index{DFT}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/fftsin}
  \caption{Diskrete Fouriertransformation von 200 diskreten
    Datenpunkten, die zwischen 0 und $2\pi$ als $\sin(x) +
    0,1\,\sin(10 x) + \xi$ erzeugt wurden, wobei $\xi$ eine
    Gauß-verteilte Pseudozufallsvariable mit Varianz $0,01$ war.  Auch
    wenn es den Ausgangsdaten im linken Graphen nicht mehr anzusehen
    ist, erlaubt die DFT, auch die kleine zusätzliche Schwingung gut
    vom Rauschen zu unterscheiden. Im Graphen rechts ist das
    Leistungsspektrum $\lvert DFT(k)\rvert^2$ gezeigt.  Man erkennt
    die Amplitudenquadrate $0,25$ bei 1 und $0,01$ bei 10, auch wenn
    letztere Frequenz durch das Rauschen etwas an Intensität verloren
    hat.}
  \label{fig:fouriersin}
\end{figure}

Bei praktischen Anwendungen sind die Integrale zur Bestimmung der
Koeffizienten oft nicht analytisch lösbar, oder die Funktion ist von
vornherein nur an diskreten Punkten gegeben, etwa weil es sich um
Messdaten handelt. In diesem Fall müssen die Integrale numerisch
approximiert werden. Wir betrachten nun also nicht mehr eine
kontinuierliche Funktion $f$, sondern Daten $f_k = f(t_k)$ mit
$t_k=k\Delta$, $k=0(1)N-1$ und Schrittweite $\Delta =
\frac{T}{N}$. Dann ist
\begin{equation}
  \hat{f}_n = \frac{1}{T}\int_0^T f(t)e^{-i n\omega t}\, dt
  \approx
  \frac{\Delta}{T}\sum_{k=0}^{N-1} f(k\Delta) e^{-i n\omega k\Delta}
  =
  \frac{1}{N}\sum_{k=0}^{N-1} f_k e^{-i\frac{2\pi}{N} n k} =: \frac{g_n}{N}.
\end{equation}
Die Koeffizienten
\begin{equation}
  \label{eq:dft}
  \text{DFT}(f_k)_n = g_n = \sum_{k=0}^{N-1} f_k e^{-i \frac{2\pi}{N} n k}
\end{equation}
werden als die \emph{diskrete Fouriertransformierte} bezeichnet, die
sehr effizient berechnet werden kann, wie wir im Folgenden sehen
werden. Analog wird die \emph{inverse diskrete Fouriertransformation}
\begin{equation}
  \label{eq:idft}
  \text{iDFT}(g_n)_k = f(t_k) = \sum_{n=0}^{N-1} \frac{g_n}{N} e^{i \frac{2\pi}{N} n k}
\end{equation}
definiert, die aus den Koeffizienten wieder die Funktion $f$ an den
diskreten Eingangspunkten $t_k$
berechnet. Abbildung~\ref{fig:fouriersin} zeigt die DFT der Summe
zweier verrauschter Sinusfunktionen. Die Frequenzen und Amplituden der
beiden Sinusfunktionen können in der Fouriertransformierten klar vom
Rauschen unterschieden werden, während die schwächere Frequenz in den
eigentlichen Daten nicht mehr zu erkennen ist.

Die Koeffizienten sind offenbar periodisch, da
\begin{equation}
  \label{eq:dftper}
  g_{n+N} = \sum_{k=0}^{N-1} f_k e^{-i\frac{2\pi}{N} (n + N) k} =
  \sum_{k=0}^{N-1} f_k e^{-i\frac{2\pi}{N} n k} \underbrace{e^{-2\pi i k}}_{=1} = g_n.
\end{equation}
Insbesondere ist $g_{-k} = g_{N-k}$, und es gibt nur $N$ echt
verschiedene Koeffizienten zu Frequenzen $n/T$. DFT-Bibliotheken
speichern die Koeffizienten daher meist als Vektor
$(g_{0},\ldots,g_{N-1})$. Für gerades $N$ kann man dies wegen der Periodizität auch als $(g_{0},\ldots,g_{N/2-1},g_{-N/2},\ldots,g_{-1})$ lesen, für ungerades $N$ als
$(g_{0},\ldots,g_{(N-1)/2},g_{-(N-1)/2},\ldots,g_{-1})$.

Ist $f$ reell, so gilt noch dazu $g_{-n} = \overline{g_{n}}$, sodass lediglich
$\lceil N/2\rceil$ Koeffizienten wirklich verschieden sind. Dabei ist offenbar $g_0$ stets reell und für gerades $N$ auch $g_{-N/2} = g_{N/2} = \overline{g_{-N/2}}$. Für ungerades $N$ gibt es also $(N-1)/2$ komplexe und einen reellen Koeffizienten, $g_0$, für gerades $N$ sind es $N/2-1$ komplexe und zwei reelle Koeffizienten, nämlich $g_0$ und $g_{-N/2}$. Das kann durch jeweils genau $N$ reelle Zahlen beschrieben werden, die Anzahl der Freiheitsgrade der Eingangsdaten bleibt also erhalten. Die Auswertung vereinfacht sich zu
\begin{equation}
  \text{iDFT}(g_n)_k = \begin{cases}
  	\frac{g_0}{N} + \sum\limits_{n=1}^{(N-1)/2-1} \Re\left[\frac{g_n}{N} e^{i \frac{2\pi}{N} n k}\right] & \text{für }N\text{ ungerade} \\
  	\frac{g_0}{N} + \sum\limits_{n=1}^{N/2-1} \Re\left[\frac{g_n}{N} e^{i \frac{2\pi}{N} n k}\right] +
	\frac{g_{-N/2}}{N} (-1)^k
	& \text{für }N\text{ gerade}.
  \end{cases}
\end{equation}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/fftalias}
  \caption{Diskrete Fouriertransformation von 14 äquidistanten
    diskreten Datenpunkten (rote Punkte links) der Funktion
    $\sin(13t)$ (rote Kurve links) im Interval $[0:2\pi]$.  Die
    Frequenz der Funktion $13/2\pi$ ist höher als die Nyquist-Frequenz
    $f_\text{Nyquist}=7/2\pi$, daher kommt es zu
    Aliasing-Artefakten. Die rekonstruierte Kurve ist links schwarz
    gepunktet eingezeichnet, ihr Spektrum rechts. Die abgetastete
    Funktion ist also scheinbar $\sin(t)$, was einer Frequenz von
    $2f_\text{Nyquist} - 13/2\pi$ entspricht.}
  \label{fig:fourieralias}
\end{figure}

Die endliche Anzahl der diskreten Fourierkoeffizienten bedeutet, dass
bei einem reellen Signal mit Schrittweite $\Delta$ die maximal
darstellbare Frequenz $f_\text{Nyquist}=\frac{1}{2\Delta}$ beträgt,
die sogenannte
\emph{Nyquist-Frequenz}\index{Nyquist-Frequenz}. Signale mit höherer
Frequenz $f$ werden zu scheinbaren Signalen niedrigerer Frequenz
\begin{equation}
  f_\text{scheinbar} = \begin{cases}
    f \bmod 2 f_\text{Nyquist} & \text{falls}\; f \bmod 2 f_\text{Nyquist}
    < f_\text{Nyquist} \\
    2f_\text{Nyquist} - f \bmod 2 f_\text{Nyquist} & \text{falls}\; f \bmod 2 f_\text{Nyquist}
    \ge f_\text{Nyquist},
  \end{cases}
\end{equation}
was auch als \emph{Aliasing} bezeichnet wird.  Diese Phänomen ist
nicht spezifisch für die Fouriertransformation, da das Aliasing auch
im Datenraum sichtbar wird, ähnlich einer \emph{Schwebung}. Sollen
analoge Signale digital weiter verarbeitet werden, kann es daher
notwendig sein, höhere Frequenzen durch analoge Tiefpassfilter zu
unterdrücken, bevor diese digitalisiert
werden. Abbildung~\ref{fig:fourieralias} illustriert dieses Problem
anhand einer sehr raschen Schwingung, die im scheinbar zu einer
langsamen Schwingung wird.

\subsection{Schnelle Fouriertransformation}
\index{Fouriertransformation>schnelle}\index{FFT}

Die Berechnung der Fouriertransformierten nach \eqref{eq:dft} ist zwar
möglich, aber ziemlich langsam --- jeder der $N$ Koeffizienten
benötigt offenbar $\O(N)$ Operationen, so dass die DFT insgesamt
$\O(N^2)$ Operationen braucht. Das limitiert für praktische
Anwendungen $N$ auf einige tausend, was für viele Anwendungen zu wenig
ist. Die DFT konnte daher nur durch die \emph{schnelle
  Fouriertransformation} (FFT) von \emph{Cooley und Tukey} zu breiter
Anwendung finden. Diese basiert auf der Beobachtung, dass für $N=2M$
\begin{align}
  \text{DFT}(f_k)_n &= \sum_{k=0}^{M-1} f_{2k} e^{-i\frac{2\pi}{2M} n\, 2k} +
  \sum_{k=0}^{M-1} f_{2k+1} e^{-i\frac{2\pi}{2M} n\, (2k + 1)}\\
  &= \text{DFT}(f_{2k})_n + e^{-i\frac{\pi}{M} n}
  \text{DFT}(f_{2k+1})_n,
\end{align}
wobei $\text{DFT}(f_{2k})_n$ den $n$-ten Koeffizienten einer DFT auf
den Datenpunkten $f_{2k}$, $k=0(1)M-1$, bezeichnet. Gemäß
\eqref{eq:dftper} ist dabei $\text{DFT}(f_{2k})_n =
\text{DFT}(f_{2k})_{n-M}$ für $n>M$.

Die Fouriertransformierte der $N$ Datenpunkte ergibt sich also als
einfache Summe von zwei
Fouriertransformierten mit lediglich der halben Menge $M$ an
Datenpunkten, wobei die ungerade Hälfte mit der \emph{Einheitswurzel}
\begin{equation}
  w^n := e^{-i\frac{2\pi}{2M} n}
\end{equation}
multipliziert wird.  Ist nun $M$ wieder durch zwei teilbar, so lassen
sich diese Fouriertransformierten ebenfalls als Summe zweier nochmals
halb so langer Fouriertransformationen darstellen. Wenn nun $N$ eine
Zweierpotenz ist, kann man so fortfahren, bis $M=1$ erreicht ist, also
$\text{DFT}(f_0)_0 = f_0$. Dabei gibt es offenbar $\log_2(N)$ viele
Unterteilungsschritte, die jeder $\O(N)$ Operationen kosten. Insgesamt
benötigt die FFT also lediglich $\O(N\log N)$ Operationen.

Schematisch funktioniert ein FFT-Schritt wie folgt:
\begin{center}
  \begin{tikzpicture}[x=2em,y=3em,>=stealth']
    \draw (0,0)  node (f0) {$f(0)$} ;
    \draw (0,-1) node (f1) {$f(\Delta)$} ;
    \draw (0,-2) node (f2) {$f(2\Delta)$} ;
    \draw (0,-3) node (f3) {$f(3\Delta)$} ;

    \draw (3,0.25) rectangle +(2,-1.5);
    \draw (4,-0.5) node {
      \begin{minipage}{4em}\centering
        Halbe\\
        FFT
      \end{minipage}
    } ;

    \draw (3,-1.75) rectangle +(2,-1.5);
    \draw (4,-2.5) node {
      \begin{minipage}{4em}\centering
        Halbe\\
        FFT
      \end{minipage}
    } ;

    \draw (8,0)  node (g0) {$g_0$} ;
    \draw (8,-1) node (g1) {$g_1$} ;
    \draw (8,-2) node (g2) {$g_2$} ;
    \draw (8,-3) node (g3) {$g_3$} ;

    \draw[->] (f0) -- (3,0);
    \draw[->] (f2) -- (3,-1);
    \draw[->] (f1) -- (3,-2);
    \draw[->] (f3) -- (3,-3);

    \draw[->] (5,0)   -- node[above,pos=0.97] {$\cdot 1$} (g0.west);
    \draw[->] (5,-2)  -- node[below,pos=0.97] {$\cdot w^0$}(g0.west);
    \draw[->] (5,-1)  -- node[above,pos=0.97] {$\cdot 1$}(g1.west);
    \draw[->] (5,-3)  -- node[below,pos=0.97] {$\cdot w^1$}(g1.west);
    \draw[->] (5,-2)  -- node[above,pos=0.97] {$\cdot 1$}(g2.west);
    \draw[->] (5,0)   -- node[below,pos=0.97] {$\cdot w^2$}(g2.west);
    \draw[->] (5,-3)  -- node[above,pos=0.97] {$\cdot 1$}(g3.west);
    \draw[->] (5,-1)  -- node[below,pos=0.97] {$\cdot w^3$}(g3.west);
  \end{tikzpicture}
\end{center}
Aufgrund ihres Aussehens wird dieses Datenpfadschema auch als
Butterfly-Schema genannt. Damit die beiden Unter-FFTs auf einem
zusammenhängenden Satz von Daten operieren können, müssen also auch
die Eingabedaten $f_k$ umsortiert werden, ebenso wie auch für die
Unter-FFTs. Man kann sich leicht überlegen, dass dabei $f_k$ auf
$f_{k'}$ sortiert wird, wobei die Bits  $k'$ in Binärdarstellung
dieselben wie von $k$ sind, nur in umgedrehter Reihenfolge.

Die FFT erlaubt also die effiziente Zerlegung einer Funktion in ihre
Schwingungskomponenten, was viele wichtige Anwendungen nicht nur in
der Physik hat. Daher gibt es eine Reihe sehr guter Implementierungen
der FFT, allen voran die "`Fastest Fourier Transform in the West"'
(FFTW, \url{http://www.fftw.org}). Selbstverständlich bietet auch NumPy
eine FFT, \scipy{numpy.fft.fft(f_k)}, mit der inversen FFT
\scipy{numpy.fft.ifft(g_n)}. Die Routinen sind so implementiert, dass
bis auf Maschinengenauigkeit $\text{iFFT}(\text{FFT}(f_k)) = f_k$.

Wichtige Anwendungsbeispiele der diskreten Fouriertransformation sind
zum Beispiel die Datenformate JPEG, MPEG und MP3, die alle drei auf
einer Abwandlung der DFT beruhen, der \emph{diskreten
  Cosinustransformation} (DCT) für reelle Daten. Bei dieser wird der
Datensatz in der Zeitdomäne so verdoppelt, dass er eine gerade
Funktion repräsentiert. Dadurch wird die Fourierreihe eine reine
Cosinusreihe mit nur reellen Koeffizienten. Die DCT ist also eine
Umwandlung reeller in reelle Zahlen. Wegen Ihrer Wichtigkeit gibt es
nicht nur extrem effiziente Implementierungen für die meisten
Prozessortypen, sondern auch spezielle Hardware.

\section{\keyword{Wavelets}}
\index{Multiskalenanalyse}

Die Fouriertransformation wird vor allem deshalb für die Kompression
von Audio- oder Bilddaten genutzt, weil sie hochfrequente von
niederfrequenten Signalen trennt und die menschlichen Sinne die
hochfrequenten Anteil meist nicht gut wahrnehmen können. Das ist
allerdings nicht ganz korrekt, tatsächlich können wir nur stark lokale
Änderungen nicht gut wahrnehmen. Dafür sind Fourierreihen an sich gar
nicht so gut geeignet, da ja auch die hochfrequenten Schwingungen
alles andere als lokal sind. Als Alternative hat sich die
\emph{Multiskalenanalyse} (MSA) oder \emph{diskrete
  Wavelettransformation} etabliert, die auch im transformierten Raum
lokal ist.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/wavelet}
  \caption{Diskrete Wavelettransformation von 200 diskreten
    Datenpunkten, die analog zu Abbildung~\ref{fig:fouriersin} zwischen
    0 und $2\pi$ als $\sin(x) + 0,1\,sin(10 x) + \xi$ erzeugt
    wurden. Für die Transformation wurden die Wavelets von $(0,1]$ auf
    den Bereich $(0,2\pi]$ gestreckt. Der linke obere Graph zeigt
    nochmals das Ausgangssignal, von rechts oben nach rechts unten
    folgen die Anteile der Stufen 0--3, also $(f,\phi_0)\phi_0 +
    \sum_{j=0}^{3} \sum_{k=0}^{2^j-1} (f,\psi_{jk})\psi_{jk}$, dann
    Stufen 4 und 5 ($\sum_{j=4}^{5} \sum_{k=0}^{2^j-1}
    (f,\psi_{jk})\psi_{jk}$) und schließlich Stufen 6--8, womit die
    Auflösung der Ausgangsdaten erreicht ist.}
  \label{fig:dwt}
\end{figure}

Anders als bei der Fouriertransformation, die eine Zerlegung in
trigonometrische Funktionen darstellt, gibt es für die MSA
verschiedene Sätze von Basisfunktionen mit verschiedenen Eigenschaften
wie Differenzierbarkeit und Lokalität. Im Folgenden soll die MSA mit
Hilfe des Haar-Wavelets dargestellt werden, dass das einfachste und
älteste bekannte Wavelet ist. Zunächst betrachten wir die
\emph{Skalierungsfunktion}
\begin{equation}
  \phi(x) = \chi_{(0,1]} =
  \begin{cases}
    1 &\text{für}\; 0 < x \le 1\\
    0 &\text{sonst}
  \end{cases}
\end{equation}
sowie das Haar-\emph{Wavelet}
\begin{equation}
  \psi(x) =
  \begin{cases}
    -1 &\text{für}\; 0 < x \le \frac{1}{2}\\
    1 &\text{für}\; \frac{1}{2} < x \le 1 \\
    0 &\text{sonst},
  \end{cases}
\end{equation}
aus denen wir die Basisfunktionen $\phi_k(x) := \phi(x - k)$ der nullten
Stufe und $\psi_{jk}(x) := 2^{j/2}\psi(2^jx-k)$ der $j$-ten Stufe
konstruieren. Durch die Skalierung mit $2^j$ werden die $\psi_{j,k}$
also immer schmaler, sind aber wegen des Vorfaktors alle normiert,
\dh $\lVert \psi_{jk} \rVert = 1$. Ebenso sind auch die $\phi_k$
normiert. Zusätzlich sind sämtliche Basisfunktionen zu einander
orthogonal, wie man sich leicht überlegt. Daher lässt sich jede
quadratintegrable Funktion $f$ wie folgt zerlegen:
\begin{equation}
  f(x) = \sum_{k\in\ZZ} (f,\phi_k)\phi_k + \sum_{j\in\NN_0}
  \sum_{k\in\ZZ} (f,\psi_{jk})\psi_{jk}
\end{equation}
Dies ist die Multiskalenanalyse von $f$. Die Koeffizienten der Stufe
$j$ werden auch Details der Stufe $j$ genannt. In der Praxis ist das
Signal durch endlich viele äquidistante Datenpunkte gegeben, analog
zur diskreten Fouriertransformation. In diesem Fall sind die Summen
endlich, da einerseits der Träger endlich ist und damit nur endlich
viele $(f,\phi_k)\neq 0$, und es andererseits keine Details unterhalb
der Auflösung des Signals gibt. Man skaliert dann die Wavelets und
Skalierungsfunktion so, dass der Abstand der Datenpunkte gerade der
halben Breite des Wavelets auf der feinsten Auflösung entspricht, und
$\phi = \phi_0$ bereits das gesamte Interval überdeckt. Für eine nur
auf $[0,1]$ nichtverschwindende Funktion, deren Werte an $2^N$ Punkten
äquidistanten Punkten bekannt ist, reduziert sich die
Multiskalenanalyse zur \emph{diskreten
  Wavelettransformation}\index{Wavelets>-transformation}
\begin{equation}
  \label{eq:idwt}
  f(x) = (f,\phi)\phi +
  \sum_{j=0}^{N-1} \sum_{k=0}^{2^j-1} (f,\psi_{jk})\psi_{jk}.
\end{equation}
Die Anzahl der Koeffizienten ist dann $1 + 1 + 2 + \cdots + 2^{N-1} =
2^N$, also genau die Anzahl der Eingabedaten. Genau wie die diskrete
Fouriertransformation bildet die Wavelettransformation $2^N$ Werte
$f(k/2^N)$ auf $2^N$ Werte $(f,\phi)$ und $(f,\psi_{jk})$ ab und
besitzt eine exakte Rücktransformation, \eqref{eq:idwt}.

Analog zur schnellen Fouriertransformation gibt es auch eine schnelle
Wavelettransformation (FWT), die sogar linearen Aufwand hat, also $\O(N)$
Schritte bei $N$ Datenpunkten benötigt. Eine einfache Implementation
der FWT und der inversen FWT für das Haar-Wavelet zeigt
Codebeispiel~\ref{lst:dwt}. Der Kern dieser Transformation liegt
darin, die Transformierte von der höchsten Detailauflösung herab
aufzubauen, und dadurch die die Integrale approximierenden Summen
schrittweise aufzubauen (\emph{Downsampling}). Für genauere
Informationen siehe zum Beispiel \textcite{daubechies92a}.

Abbildung~\ref{fig:dwt} zeigt einige Detailstufen der
Wavelet-Zerlegung der verrauschten Sinusfunktionen analog
Abbildung~\ref{fig:fouriersin}. Auch hier lässt sich das Rauschen auf
den höheren Detailstufen gut vom Nutzsignal trennen, allerdings kann
die Oberschwingung nicht detektiert werden. Das hängt allerdings vor
allem daran, dass das Haar-Wavelet nicht sehr geeignet ist, da es
nicht glatt ist, im Gegensatz zum Nutzsignal. Daher sind in den
meisten Fällen glatte Wavelets besser geeignet. Das bekannteste
Beispiel von glatten Wavelets sind die Daubechies-Wavelets, die
daneben auch einen kompakten Träger haben, also stark lokalisiert
sind. Mit solchen Wavelets lassen sich sogar reale Musikdaten in
Akkorde zurücktransformieren. Auch der JPEG-Nachfolger JPEG2000
basiert auf einer Wavelettransformation statt einer
Cosinustransformation, allerdings mit
Cohen-Daubechies-Feauveau-Wavelets.

\lstinputlisting[style=floating,firstline=10,
caption={Diskrete Wavelettransformation und ihre Inverse als
  Python-Code. Die Länge der Eingabedaten muss eine Zweierpotenz $2^N$
  sein. Die Details sind in einem Vektor $c$ gespeichert, in der Form
  $c=((f,\phi_0)$, $(f, \psi_{00})$, $(f, \psi_{10})$, $(f, \psi_{11})$,
  $(f, \psi_{20})$, $(f, \psi_{21})$, $(f, \psi_{22}), \ldots,
  (f, \psi_{N-1,2^{N-1}-1}))$.
},
label=lst:dwt]{dwt.py}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "padc"
%%% TeX-PDF-mode: t
%%% End: 
