% Dies ist Teil der Vorlesung Physik auf dem Computer, SS 2012,
% Axel Arnold, Universitaet Stuttgart.
% 
% Dieses Werk ist unter einer Creative Commons-Lizenz vom Typ
% Namensnennung-Weitergabe unter gleichen Bedingungen 3.0 Deutschland
% zugänglich. Um eine Kopie dieser Lizenz einzusehen, konsultieren Sie
% http://creativecommons.org/licenses/by-sa/3.0/de/ oder wenden Sie sich
% schriftlich an Creative Commons, 444 Castro Street, Suite 900, Mountain
% View, California, 94041, USA.

\chapter{Nichtlineare Gleichungssysteme}
\index{Nullstellensuche}
\index{Fixpunktsuche}

Im zweiten Kapitel hatten wir uns mit der Lösung linearer
Gleichungssysteme beschäftigt, die ja eine wesentliche Grundlage der
numerischen Mathematik darstellen. Allerdings tauchen in der Praxis,
besonders in der Physik, leicht auch nichtlineare Gleichungssysteme
auf. In diesem Fall kann man meist keine allgemeine Aussage über
Existenz und Anzahl der Lösungen machen und auch keine exakten
Verfahren zur Lösung angeben.

Nichtlineare Gleichungssysteme werden typischerweise in zwei Formen
betrachtet. Sei eine Funktion $f:M\subseteq\RR^n\to\RR^n$ gegeben. Dann suchen
wir die \emph{Nullstellen}
\begin{equation}
  \label{eq:nullstellen}
  x, \quad\text{so dass}\; f(x) = 0,
\end{equation}
also die Lösungen zur Gleichung $f(x) = 0$. Man beachte, dass anders
als im Fall der linearen Gleichungssysteme gefordert wird, dass der
Bildraum wie auch der Ursprungsraum $M$ zu einem Vektorraum derselben
Dimension $n$ gehören. Ohne diese Voraussetzung ist eine eindeutige
Lösung im Allgemeinen unmöglich. Anders als im Falle der linearen
Gleichungssysteme ist es hier auch nicht ohne Weiteres möglich, den
Lösungsraum anzugeben, falls die Lösung nicht eindeutig
ist. Tatsächlich kann der Lösungsraum ja eine beliebig komplexe
Mannigfaltigkeit innerhalb $M$ darstellen, die dann gar nicht
geschlossen parametrisiert werden kann. Das macht die numerische
Bestimmung dieser Lösungsmannigfaltigkeit sehr schwierig.

Alternativ können wir für eine Funktion $g:M\subseteq\RR^n\to\RR^n$ die
\emph{Fixpunkte} suchen. Diese sind
\begin{equation}
  \label{eq:fixpunkt}
  x, \quad\text{so dass}\; g(x) = x,
\end{equation}
also die Lösungen der Gleichung $g(x) = x$. Eine Fixpunktgleichung
lässt sich natürlich stets auch als Nullstellenproblem mit $f(x) =
g(x) - x$ formulieren und umgekehrt.

Die beiden Formulierungen unterscheiden sich allerdings im natürlichen
Lösungsansatz. Die Nullstellengleichung \eqref{eq:nullstellen} ähnelt
dem linearen Gleichungssystem \eqref{eq:lgs}. Das
\emph{Newtonverfahren} beruht auf einer lokalen Linearisierung und dem
Lösen dieses linearen Gleichungssystems. Die Fixpunktgleichung
hingegen legt nahe, den Fixpunkt durch \emph{sukzessive Substitution}
zu suchen: $x_0\to g(x_0)\to g(g(x_0))\to\ldots$.

\section{Sukzessive Substitution}
\index{sukzessive Substitution}
\index{Fixpunktsuche}

Eine Abbildung $g:M\to M$ mit $M\subset\RR^n$ heißt Lipschitz-stetig
(L-stetig), falls es ein $L\in\RR$ gibt, so dass
\begin{equation}
  \norm{g(x) - g(y)} \le L \norm{x - y}\quad\forall x,y\in M.
\end{equation}
Alle auf $M$ differenzierbaren Funktionen mit beschränkter Ableitung
sind L-stetig, wenn ihre Ableitung beschränkt ist. Die
Lipschitzkonstante ergibt sich aus dem Mittelwertsatz zu $L=\max_{x\in
  M}\norm{g'(x)}$.  Es gibt allerdings noch mehr L-stetige Funktionen,
zum Beispiel die in 0 nicht differenzierbare Betragsfunktion, die auf
ganz $\RR$ L-stetig mit $L = 1$ ist. Auf der anderen Seite ist
offenbar jede L-stetige Funktion auch stetig, \dh\,, die L-stetigen
Funktionen sind eine eigene Klasse zwischen den stetigen und
differenzierbaren Funktionen. Dabei ist es nicht wesentlich, welche
Norm $\norm{\cdot}$ genutzt wird. Sie kann für das Problem geeignet
gewählt werden, sofern sie die üblichen Bedingungen an eine Norm, wie
etwa die Dreiecksungleichung, erfüllt.

\index{Banachscher Fixpunktsatz} Hat eine Funktion $g:M\to M$ eine
Lipschitz-Konstante $L<1$, so heißt $g$ \emph{kontrahierend}, weil
zwei verschiedene Punkte durch die Abbildung stets näher aneinander
geschoben werden. Wir betrachten nun einen beliebigen Startpunkt
$x_0\in M$ und definieren damit die Folge der \emph{sukzessiven Substitution}:
\begin{equation}
  x_{n} := g(x_{n-1}) \quad\text{für}\; n\ge 1.
\end{equation}
Dann gilt für alle $n,m\in\NN$ der \emph{Banachsche Fixpunktsatz}
\begin{align}
  \label{eq:banach}
  \norm{x_{n+m} - x_n} \,=\, &\norm{\sum_{k=0}^{m-1} x_{n+k+1} - x_{n+k}}
  \,\le\, \sum_{k=0}^{m-1} \norm{x_{n+k+1} - x_{n+k}}\nonumber\\
  \,=\, &\norm{g(g(\ldots g(x_{n+1}))) - g(g(\ldots g(x_{n})))}
  + \cdots\nonumber\\
  &+ \norm{g(g(x_{n+1})) - g(g(x_{n}))}
  + \norm{g(x_{n+1}) - g(x_{n})} + \norm{x_{n+1} - x_{n}}\nonumber\\
  \le\, &\sum_{k=0}^{m-1} L^k \norm{x_{n+1} - x_{n}}
  \,\le\, \frac{1}{1-L}\norm{x_{n+1} - x_{n}} \le
  \frac{L^n}{1-L}\norm{g(x_0) - x_0}.
\end{align}
Die sukzessive Substitution definiert also eine Cauchyfolge, die in
$M$ konvergiert, sofern $M$ abgeschlossen ist (\zb $M=\RR^n$ oder $M$
Einheitskugel). Für den Grenzwert $\overline{x}$ dieser Folge gilt
\begin{equation}
  \overline{x} = \lim_{n\to\infty}x_{n+1} = \lim_{n\to\infty}g(x_{n})
  = g(\overline{x}),
\end{equation}
er ist also ein Fixpunkt.

Wir betrachten nun zwei Fixpunkte $\overline{x}$ und $\overline{y}$. Dann gilt
\begin{equation}
  \norm{\overline{x} - \overline{y}} = \norm{g(\overline{x}) -
    g(\overline{y})} \le L \norm{\overline{x} - \overline{y}} \implies
  \overline{x} = \overline{y}.
\end{equation}
Das bedeutet, dass es nur genau einen Fixpunkt $\overline{x}$ von $g$
in $M$ gibt, und dass die sukzessive Substitution für jeden Startwert
\emph{global} gegen $\overline{x}$ konvergiert. \eqref{eq:banach}
gibt auch eine \textit{a priori}-Abschätzung des Fehlers:
\begin{equation}
  \norm{\overline{x} - x_n} \le \frac{L^n}{1-L}\norm{g(x_0) - x_0}
\end{equation}
sowie eine Abschätzung der Konvergenzrate:
\begin{equation}
  \frac{\norm{x_{n+1} - \overline{x}}}{\norm{x_n - \overline{x}}}
  = \frac{\norm{g(x_n) - g(\overline{x})}}{\norm{x_n - \overline{x}}}
  \le L < 1.
\end{equation}
Die sukzessive Substitution konvergiert also linear. Das Verfahren
wird abgebrochen, wenn $\norm{x_{n+1} - x_n}$ hinreichend klein ist,
da dann nach \eqref{eq:banach} auch
\begin{equation}
  \norm{\overline{x} - x_n} \le \frac{1}{1-L}\norm{x_{n+1} - x_n}.
\end{equation}
Offenbar konvergiert die sukzessive Substitution um so schneller, je
kleiner die Lipschitzkonstante $L$ ist.

Neben der globalen Konvergenzeigenschaft konvergiert die sukzessive
Substituion auch lokal: ist $g:\RR^n\to\RR^n$ eine differenzierbare
Funktion und hat einen Fixpunkt $\overline{x}$ mit $\norm{g'(x)} < 1$,
so gibt es eine Umgebung des Fixpunktes, in dem die sukzessive
Substitution gegen diesen Fixpunkt konvergiert.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/banach}
  \caption{Sukzessive Substitution mit Funktion $g(r) = e^{-r}/\phi_0$
    mit $\phi_0=2$ (links) und $\phi_0=1/4$ (rechts). Blau
    durchgezogen ist die Funktion $g$, die Winkelhalbierende ist
    schwarz dargestellt. Die Punkte auf der Winkelhalbierenden
    markieren die Punkte $(x_1,x_1)$, $(x_2,x_2)$ \usw\,, durch die das
    Lot auf $g$ gefällt wird, um den nächsten Punkt der sukzessiven
    Substitution zu erhalten. Im linken Graph sind die ersten sieben
    Glieder dargestellt, und die lineare Konvergenz ist gut zu
    sehen. Im rechten Graph konvergiert das Verfahren nicht, da die
    Lipschitzkonstante $L$ größer als 1 ist.}
  \label{fig:banach}
\end{figure}

\subsection{Beispiel}

Als Beispiel für eine Anwendung des Banachschen Fixpunktsatzes
betrachten wir die dimensionslose Form des Yukawa- oder
Debye-Hückel-Potentials $\phi(r) = e^{-r}/r$. Wir fragen uns, wann für
welches $r$ dieses Potential einen gegeben Wert $\phi_0$ annimmt. Das
führt zu der Fixpunktgleichung
\begin{equation}
  g(r) = \frac{e^{-r}}{\phi_0} = r
\end{equation}
Die linke Seite ist eine auf $[0,\infty)$ L-stetige Funktion mit
$L=1/\phi_0$, wie man durch Ableiten leicht sieht.

Abbildung~\ref{fig:banach} zeigt die sukzessive Substitution für
$g(r)$. Graphisch lässt sich das Verfahren visualisieren, indem in
jeder Iteration der Funktionswert $x_{n+1} = y =g(x_{n})$ an der
Winkelhalbierenden $y=x$ auf die $x$-Achse zurückgespiegelt wird. Im
linken Graph ist $\phi_0=2$ und damit $L=1/2$, so dass die sukzessive
Substitution linear konvergiert. Für das letzte abgebildete
Glied, $x_7$, gilt $\abs{x_7-\overline{x}} \le 1/2^8 = 1/256$. Im
rechten Graph ist $\phi_0=1/4$ und damit $L=4$. Insbesondere ist auch
im Fixpunkt $g'(\overline x)>1$. Abgebildet sind die ersten zehn
Glieder der sukzessiven Substitution, die hier nicht mehr
konvergiert, obwohl wir nahe am Fixpunkt gestartet haben.

\section{Newtonverfahren in einer Dimension}
\index{Newtonverfahren}
\index{Nullstellensuche}

Nachdem wir bis jetzt die sukzessive Substitution zur Bestimmung von
Fixpunkten betrachtet haben, geht es nun um die Nullstellensuche. Sei
also zunächst eine stetig differenzierbare Funktion $f:[a,b]\to\RR$
gegeben und deren Nullstellen $x$, $f(x) = 0$, gesucht. Ähnlich wie
bei der sukzessiven Iteration starten wir mit einem Startwert
$x_0$. Um uns nun der Nullstelle der Funktion zu nähern, linearisieren
wir in der aktuellen Näherung $x_n$ und lösen nach der Nullstelle
$x_{n+1}$ auf:
\begin{equation}
  x_{n+1} = g(x_n) := x_n - \frac{f(x_n)}{f'(x_n)}\quad\text{für}\; n\ge 0,
\end{equation}
wobei wir annehmen, dass $f'(x)\neq 0$ auf $[a,b]$.  Für eine
Nullstelle $\overline{x}$ von $f$ gilt offenbar $g(\overline{x}) =
\overline{x}$, \dh wir suchen einen Fixpunkt von $g$, den wir wieder
durch sukzessive Substitution annähern können. Man bricht das
Verfahren wie auch die sukzessive Substitution ab, wenn
$\abs{x_{n}-x_{n-1}}$ bzw. $\abs{f(x_n)}$ hinreichend klein sind.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/newton}
  \caption{Newtonverfahren für die Funktion $f(r) = e^{-r}/r -
    \phi_0$. Wie schon beim Graphen zur sukzessiven Substitution ist
    links $\phi_0=2$, rechts $\phi_0=1/4$, allerdings konvergiert das
    Newtonverfahren für beide Werte. Blau dargestellt ist $f$, die
    roten, dicken Linien stellen die Tangenten dar, deren Nullstellen
    die neuen Näherungen für die gesuchte Nullstelle von $f$ sind.}
  \label{fig:newton}
\end{figure}

Ist nun $f$ sogar zweifach stetig differenzierbar, so gilt
\begin{equation}
  g'(x) = 1 - \frac{f'(x)^2 - f(x)f''(x)}{f'(x)^2} =
  \frac{f(x)f''(x)}{f'(x)^2} \implies g'(\overline{x}) = 0.
\end{equation}
Das Newtonverfahren konvergiert also zumindest lokal gegen einen
Fixpunkt $\overline{x}$ von $g$ beziehungsweise eine Nullstelle von
$f$. Tatsächlich konvergiert das Verfahren wenigstens quadratisch,
wenn $f$ zweifach differenzierbar ist, da
\begin{align}
  x_{n+1} - \overline{x} = \frac{(x_n - \overline{x})f'(x_n) -
    f(x_n)}{f'(x_n)}
  = \frac{(x_n - \overline{x})}{f'(x_n)}\left( f'(x_n) -
    \frac{f(x_n) - f(\overline{x})}{x_n - \overline{x}}\right)\nonumber\\
  = \frac{(x_n - \overline{x})}{f'(x_n)}\left( f'(x_n) -
    f'(\xi')\right)
  = \frac{(x_n - \overline{x})(x_n - \xi')}{f'(x_n)}f''(\xi)
\end{align}
und somit
\begin{equation}
  \frac{\abs{x_{n+1} - \overline{x}}}{\abs{x_n - \overline{x}}^2}
  \le \frac{\max_{\xi\in [a,b]} \abs{f''(\xi)}}{\min_{\xi\in [a,b]} \abs{f'(\xi)}}
\end{equation}
Ist $f$ nur differenzierbar, so lässt sich ähnlich zeigen, dass das
Newtonverfahren superlinear konvergiert. Das Newtonverfahren
konvergiert also in jedem Fall schneller als die sukzessive
Substitution, erfordert allerdings eine mindestens stetig
differenzierbare Funktion.

Bis jetzt haben wir nur die lokale Konvergenz des
Newton-Verfahrens. Ist die Zielfunktion $f\in C^1([a,b])$ allerdings
konvex bzw.\ konkav, also $f'$ monoton wachsend bzw.\ fallend und
beschränkt, und hat $f$ eine Nullstelle in $[a,b]$, so kann man
zeigen, dass das Newtonverfahren global gegen eine Nullstelle
$\overline{x}$ von $f$ in $[a,b]$ konvergiert. Dabei ist das Verfahren
nach dem ersten Schritt monoton, \dh entweder $x_1\le
x_2\le\ldots\le\overline{x}$ oder $x_1\ge
x_2\ge\ldots\ge\overline{x}$.

In SciPy gibt es natürlich auch das Newtonverfahren sowie einige
verbesserte Algorithmen. Das Newtonverfahren ist als
\scipy{scipy.optimize.newton(f, x0, df)} implementiert, wobei \argd{f}
und \argd{df} die Funktion $f$ und ihre Ableitung $\nicefrac{df}{dx}$
angeben, und \argd{x0} der Startwert des Newtonverfahrens.

\subsection{Beispiel}

Wir betrachten wieder die Aufgabe $e^{-r}/r = \phi_0$, bzw. $f(r) =
e^{-r}/r - \phi_0 = 0$. Die Ableitung dieser Funktion ist $-e^{-r}(1 +
r)/r^2$, $f$ fällt also monoton. Daher konvergiert das Newtonverfahren
global und monoton, wie in Abbildung~\ref{fig:newton} zu sehen. Im
linken Graphen ist $r_0<\overline{r}$, daher startet das Verfahren
sofort monoton. Im rechten Graphen ist $r_0 < \overline{r}$. Hier wird
im ersten Schritt $r_1 < \overline{r}$, und erst dann wächst die
Näherung wieder monoton. In jedem Fall konvergiert das
Newtonverfahren, anders als die sukzessive Substitution, für beide
Werte von $\phi_0$ innerhalb weniger Schritte zuverlässig gegen die
Nullstelle.

\subsection{Wurzelziehen}

Wir betrachten die Gleichung $f(x) = x^k - a$ auf der positiven
Halbachse. Dann konvergiert für jeden Startwert $x_0>0$ das
Newtonverfahren
\begin{equation}
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} = \left(1 -
    \frac{1}{k}\right) x_n + \frac{1}{k} \frac{a}{x_n^{k-1}}
\end{equation}
gegen die einzige Nullstelle von $f$, nämlich die $k$-te Wurzel aus
$a$. Sinnvollerweise wählt man daher $x_0=a$ als Startwert. Für $k=2$
ergibt sich das \emph{Heron-Verfahren} $x_{n+1} = \frac{1}{2}\left(x_n
  + \frac{a}{x_n}\right)$, das bereits im 2.\ Jhdt.\ vor Christus zum
Wurzelziehen benutzt wurde.

Für die Wurzel aus $a=2$ sind die ersten 5 Schritte des Heronverfahrens:
\begin{center}
  \begin{tabular}{r|l|l}
    Schritt $n$ & $x_n$ & Anzahl korrekter Stellen \\\hline
    0 & 1.000000000000000 & 1 \\
    1 & 1.500000000000000 & 1 \\
    2 & 1.416666666666667 & 2 \\
    3 & 1.414215686274510 & 5 \\
    4 & 1.414213562374690 & 11 \\ 
    5 & 1.414213562373095 & 15
  \end{tabular}
\end{center}
Mit der auf Rechnern üblichen doppelten Genauigkeit ist das Verfahren
damit auskonvergiert.

Die Anzahl der Rechenoperationen für $n$ Schritte entspricht der
Auswertung eines Polynoms mit $3n/2$ Koeffizienten. Wäre man zum
Beispiel nur an der Wurzel im Bereich $[0,5]$ interessiert und würde
hierzu ein interpolierendes Polynom mit 7 Chebyshev-Stützstellen
nutzen, wäre $\sqrt{2}\approx 1.40966$ mit gerade einmal einer
korrekten Stelle. Mit einer Taylorentwicklung um 1 würde es etwas
besser. Bei 7 Termen liefert diese $\sqrt{2}\approx 1.4214$ mit 2
korrekten Stellen.

Für $k=-1$ wird aus der Wurzelaufgabe eine Division, da wir die
Nullstelle der Funktion $f(x) = \frac{1}{x} - a$ suchen. Die Lösung
kann nur mit Hilfe der Grundrechenarten durch die Iteration
\begin{equation}
  x_{n+1} = x_n - \frac{f(x_n)}{f'(x_n)} = 2x_n - a x_n^2 
\end{equation}
bestimmt werden. Allerdings ist die Ableitung von $a/x$ unbeschränkt,
daher konvergiert das Verfahren nur für Startwerte, die hinreichend
nah an der Lösung sind. Wie man sich in diesem Fall leicht überlegt,
konvergiert das Verfahren nur für $x_o\in (0, 2/a)$, was schwierig zu
erfüllen ist, ohne $a^{-1}$ bereits zu kennen.

\subsection{Nullstellen von Polynomen}

Ist $p$ ein Polynom, so lassen sich dessen Nullstellen (approximativ)
mit Hilfe des Newtonverfahrens bestimmen:
\begin{equation}
  x_{n+1} = x_n - \frac{p(x_n)}{p'(x_n)},
\end{equation}
wobei $p(x_n)$ und $p'(x_n)$ durch ein modifiziertes Hornerschema
bestimmt werden können. Die folgende Routine berechnet effizient
$x_{n+1}$ aus $x_n$:%
\lstinputlisting[firstline=10]{horner_newton.py}%
Das Newtonverfahren liefert natürlich nur eine Nullstelle des
Polynoms. Durch die Polynomdivision, wieder mit Hilfe des
Hornerschemas wie in Abschnitt~\ref{sec:horner}, lässt sich diese
aber abspalten und das Newtonverfahren erneut starten, bis alle
Nullstellen gefunden sind.

\section{\keyword{Sekantenmethode}}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/sekanten}
  \caption{Sekantenmethode für die Funktion $f(r) = e^{-r}/r -
    \phi_0$. Wie zuvor ist links $\phi_0=2$, rechts $\phi_0=1/4$.
    Blau dargestellt ist wieder $f$, die roten, gestrichelten dicken
    Linien stellen die Sekanten dar, deren Nullstellen die neuen
    Näherungen für die gesuchte Nullstelle von $f$ sind. Durchgezogen
    ist der Abschnitt der Sekante durch $x_{n-1}$ und $x_n$, der von $x_n$
    zu $x_{n+1}$ führt.}
  \label{fig:sekanten}
\end{figure}

In vielen Fällen ist es nicht einfach oder unmöglich, die Ableitung
einer Funktion zu bestimmen. In diesem Fall kann man die Ableitung
durch die dividierte Differenz annähern, wobei nun zwei Startpunkte
$x_0$ und $x_1$ gebraucht werden. Dies führt dazu, dass die
Näherungsfunktion keine Tangente mehr ist, wie beim Newtonverfahren,
sondern im Allgemeinen eine Sekante. Daher rührt der Name der
\emph{Sekantenmethode}
\begin{equation}
  x_{n+1} = x_n - f(x_n)\frac{x_n - x_{n-1}}{f(x_n) - f(x_{n-1})},
\end{equation}
die nicht mehr quadratisch, aber wenigstens superlinear konvergiert.

Das Bestimmen der Ableitung von $f$ ist immer dann unmöglich, wenn $f$
sehr komplex ist. Ein Extremfall wäre eine
Molekulardynamik-Computersimulation, bei der zum Beispiel in
Abhängigkeit vom aktuellen Volumen $V$ der mittlere Druck $P(V)$ in
einem gegebenen System bestimmt wird. Den mittleren Druck nach $V$
abzuleiten, ist vollkommen aussichtslos, wenn die Wechselwirkungen
zwischen den Teilchen hinreichend komplex sind. Dabei ist es von
großem Interesse, dasjenige Volumen zu bestimmen, für das $P(V)$
gleich einem vorgegebenen Außendruck $P_0$ ist, denn dies ist die
natürliche Bedingung im isobarischen Ensemble. Dies führt aber zur
Nullstellengleichung $P(V)-P_0 = 0$.

Als Beispiel für die Sekantenmethode soll ein weiteres Mal die
Funktion $f(r) = e^{-r}/r - \phi_0$ dienen. Für diese zeigt
Abbildung~\ref{fig:sekanten} die ersten paar Schritte der
Sekantenmethode. Unter geeigneten Umständen, nämlich, wenn die
angenäherte Tangente hinreichend gut mit der tatsächlichen
über einstimmt, konvergiert die Sekantenmethode praktisch genauso gut wie
das normale Newtonverfahren. Sind die Startpunkte allerdings ungünstig
gewählt, wie im rechten Beispiel, so kann es passieren, dass diese
abwechselnd um die Nullstelle liegen, und damit nicht mehr monoton
konvergieren.

In SciPy implementiert \scipy{scipy.optimize.newton(f, x0)} (also bei
Nichtangabe der Ableitung) die Sekantenmethode. \argd{f} ist wieder
die Funktion, deren Nullstelle gesucht wird, und \argd{x0} ein
Startwert. Der zweite wird hardcodiert in einer Umgebung von \argd{x0}
gewählt, kann also vom Benutzer nicht gesetzt werden.

\section{\keyword{Bisektion}}
\index{Bisektionsverfahren}

Wie wir gesehen haben, konvergiert das Newtonverfahren und seine
Varianten sehr schnell, allerdings oft nur unter der Voraussetzung,
dass der Startwert hinreichend nah an einer Nullstelle liegt. Wie aber
kann man einen solchen Startwert finden? Hierfür wird ein langsameres,
robusteres Verfahren gebraucht, zum Beispiel das
\emph{Bisektionsverfahren} in einer Dimension.

Sei also wieder $f\in C([a,b])$ eine stetige Funktion, und
$f(a)f(b)<0$. Dann hat $f$ gemäß Mittelwertsatz wenigstens eine
Nullstelle im Intervall $[a,b]$, die wir suchen. Dazu setzen wir
zunächst $a_0=a$ und $b_0=b$. Dann betrachten wir den
Intervallmittelpunkt
\begin{equation}
  m_{n} = \frac{a_n + b_n}{2}.
\end{equation}
Ist $f(m_n) = 0$, so ist
$m_n$ die gesuchte Nullstelle, und das Verfahren bricht ab. Ist
$f(m_n)f(a_n) < 0$, hat die Funktion also bei $m_n$ und $a_n$
unterschiedliche Vorzeichen, so muss eine Nullstelle im halb so
großen Intervall $a_1=a_0$, $b_1=m_0$
liegen, mit dem wir nun weiter verfahren. Andernfalls ist
notwendigerweise $f(m_n)f(b_n) < 0$, und die Nullstelle ist im neuen
Intervall $a_1=m_0$, $b_1 = b_0$. Ist $b_n - a_n$ kleiner als die
gewünschte Genauigkeit für die Nullstelle, bricht man ebenfalls
ab. $m_n$ ist dann eine Näherung für die Nullstelle mit der gewünschten
Genauigkeit.

Das Verfahren halbiert in jedem Schritt die Intervallgröße und damit
auch den maximalen Abstand der Näherung zur tatsächlichen
Nullstelle $\overline{x}$. Es gilt also
\begin{equation}
  \frac{\abs{m_{n+1} - \overline{x}}}{\abs{m_{n} - \overline{x}}}\le \frac{1}{2},
\end{equation}
das Verfahren konvergiert also nur linear, dafür aber global.

Der Vorteil der Bisektion ist, dass diese Methode im Allgemeinen
auch noch einigermaßen zufriedenstellend funktioniert, falls nicht nur
die Ableitungen unbekannt sind, sondern sogar die Funktionswerte gar
nicht genau bekannt sind. Dies wäre zum Beispiel bei der bereits
angesprochenen Messung des Drucks in einer Simulation der Fall, da
dieser starken Schwankungen unterliegt. Dadurch kann man meist den
Druck in akzeptabler Rechenzeit nicht so gut mitteln, dass dessen
Messfehler vernachlässigt werden kann. Die Bisektion muss in diesem
Fall schon abgebrochen werden, sobald der Funktionswert an einer der
Grenzen innerhalb des Fehlerbalkens Null ist.

In der Praxis kombiniert man die Bisektion mit dem Newtonverfahren, indem
zunächst einige Schritte des Bisektionsverfahrens durchgeführt
werden, und dann vom Intervallmittelpunkt aus das
Newtonverfahren. Konvergiert dieses, so ist man fertig. Läuft das
Newtonverfahren aus dem Bisektionsinterval heraus,
verkleinert man dieses durch weitere Bisektionsschritte, und versucht dann
erneut, das Newtonverfahren anzuwenden.

In SciPy implementiert \scipy{scipy.optimize.bisect(f, a, b)} mit der
Funktion \argd{f} und Intervallgrenzen \argd{a} und \argd{b} die
Bisektion. \scipy{scipy.optimize.brentq(f, a, b)} implementiert die
Brentsche Methode, die unter den selben Bedingung wie die Bisektion
konvergiert, aber meist wesentlich schneller.

\subsection{Beispiel}
In Abbildung~\ref{fig:bisektion} dient uns ein letztes Mal die
Funktion $f(x) = e^{-r}/r - 2$ als Beispiel für die Bisektion. Mit
sieben Schritten schließt diese bei Startinterval $[0,1,\,1]$ die
Nullstelle bis auf $[0,3391,\,0,3461]$, als etwa $10^{-2}$ genau
ein. Zum Vergleich: das Newtonverfahren mit Startwert $0,05$ erreicht
in sieben Schritten eine Genauigkeit von etwa $10^{-5}$, und selbst
die Sekantenmethode $10^{-3}$.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/bisektion}
  \caption{Links: Bisektion für die Funktion $f(r) = e^{-r}/r -
    \phi_0$, hier nur mit $\phi_0=2$, da die genaue Form der Funktion
    für diese Methode unerheblich ist. Blau dargestellt ist wieder
    $f$, die grünen Balken markieren die nacheinander generierten,
    kleiner werdenden Intervalle $[a_n, b_n]$. Die gestrichelten,
    schwarzen Linien markieren die Intervallmitten $m_n$, die im
    nächsten Schritt eine der beiden Intervallgrenzen werden. Rechts:
    Regula falsi für dieselbe Funktion. Die Funktion ist streng
    konvex, daher reagiert die Regula falsi lethargisch und ist der
    Bisektion unterlegen; die Intervalle verkürzen sich praktisch
    nicht mehr, da die untere Anfangsgrenze $1/4$ nie versetzt wird,
    nur die obere Grenze.}
  \label{fig:bisektion}
\end{figure}

\section{\keyword{Regula falsi}}

Sowohl die Bisektion wie auch die Sekantenmethode benötigen zwei
Startwerte. Daher liegt es nahe, die Verfahren zu kombinieren.  Bei
der \emph{Regula falsi} wird wie bei der Bisektion ein Intervall
$[a,b]$ schrittweise verkleinert so dass dieses stets wenigstens eine
Nullstelle enthält.  Anders als bei der Bisektion wird der neue
Randpunkt $m$ allerdings nicht einfach als Intervallmitte bestimmt,
sondern mit Hilfe der Sekantenmethode, mit den beiden
Intervallrandpunkten $a$ und $b$ als Stützstellen:
\begin{equation}
  m = b - f(b)\frac{b - a}{f(b) - f(a)} =
  \frac{f(b)a - f(a)b}{f(b) - f(a)}.
\end{equation}
Da $f(a)f(b)<0$, schneidet die Sekante die Nulllinie immer im
Intervall.  Dadurch verkleinert sich das Intervall schneller als beim
Bisektionsverfahren, sofern die Nullstelle der Sekante dicht an der
echten Nullstelle liegt. 

Leider gilt dies aber nur, wenn die Funktion im Intervall eine
Wendepunkt hat. Ist dies nicht der Fall, wenn die Funktion im
Intervall also konkav oder konvex ist, dann wird die Regula
falsi sehr langsam, was auch als \emph{Lethargie} bezeichnet wird.
Der Grund ist, dass eine der beiden Intervallgrenzen sehr dicht an die
Nullstelle rückt und im folgenden nur noch diese Grenze minimal
verbessert wird, während die zweite Grenze unverändert bleibt. Dies
ist zum Beispiel in Abbildung~\ref{fig:bisektion} rechts gezeigt.

Unglücklicherweise gibt es jedoch -- außer wenn die Funktion genau an
der Nullstelle einen Wendepunkt hat -- immer eine Umgebung um die
Nullstelle, in der die Funktion streng konkav oder konvex ist, so dass
die Regula falsi immer ab einer gewissen Genauigkeit lethargisch
konvergiert, daher ist die einfache Bisektion in der Regel schneller
als die Regula falsi. Die Regula falsi ist nur dann die schnellste und
stabilste Nullstellensuche, falls die Ableitung der Funktion nicht
bekannt ist, man aber weiß, dass die Funktion einen Wendepunkt in der
Nullstelle hat.

Die Regula falsi ist ein gutes Beispiel dafür, dass die Kombination
zweier schneller und robuster Verfahren keineswegs immer ein
schnelleres oder robusteres Verfahren liefert.

\section{Newtonverfahren in mehreren Dimensionen}
\index{Newtonverfahren>in mehreren Dimensionen}
\index{Gleichungssysteme>nichtlineare}

Bis jetzt haben wir das Newtonverfahren nur für eindimensionale
Funktionen betrachtet. Im Mehrdimensionalen funktioniert das Verfahren
aber sehr ähnlich, wobei die Ableitung zur Jacobimatrix wird.

Sei also $f\in C^1(M, \RR^n)$ eine stetig differenzierbare Abbildung
von $M\in\RR^n$ in den $\RR^n$. Wir suchen nun eine Nullstelle
$\overline{x}\in D$, \dh eine Lösung des nichtlinearen
Gleichungssystems $f(x) = 0$.

Wie schon im Eindimensionalen starten wir mit einer Näherung
$x^{(0)}\in M$, und berechnen die nächste Näherung $x^{(1)}$ durch
Linearisieren von $f$ in $x^{(0)}$. Die Linearisierung ergibt sich aus
der Taylorentwicklung:
\begin{equation}
  \label{eq:linnewton}
  F(x^{(1)}) \,\dot{=}\, F(x^{(0)}) +
  F'(x^{(0)})\left(x^{(1)}-x^{(0)}\right),
\end{equation}
wobei
\begin{equation}
  F'(x) = \left(\frac{d}{dx_j}F_k(x)\right)_{k,j} = 
  \begin{pmatrix}
    \frac{d}{dx_1}F_1(x) & \ldots & \frac{d}{dx_n}F_1(x)\\
    \vdots               &        & \vdots \\
    \frac{d}{dx_1}F_n(x) & \ldots & \frac{d}{dx_n}F_n(x)
  \end{pmatrix}
\end{equation}
die \emph{\keyword{Jacobimatrix}} von $f$ an der Stelle $x$ bezeichnet.
Die neue Näherung $x^{(1)}$ suchen wir als Nullstelle der
Linearisierung \eqref{eq:linnewton}, also aus der Bedingung
$F(x^{(1)})\stackrel{!}{=} 0$. Da wir ja damit nur die
linearisierte Gleichung gelöst haben, linearisieren wir erneut im
neuen Punkt $x^{(1)}$, und so weiter. Ein Schritt des Newtonverfahrens
ist dann also
\begin{equation}
  x^{(i+1)} = x^{(i)} + d^{(i)}\quad\text{mit}\;
  F'(x^{(i)})\,d^{(i)} = -F(x^{(i)}).
\end{equation}
Die \emph{Newtonkorrektur} $d^{(i)}$ wird aus der Lösung eines
linearen Gleichungssystems gewonnen, zum Beispiel mit Hilfe der
Gaußelimination. Allerdings ist $f'$ im Allgemeinen vollbesetzt, daher
verwendet man normalerweise schnellere, approximative Verfahren, die
wir später kennenlernen werden. Ist $F'(x^{(i)})$ in einem Schritt
singulär, so bricht das Verfahren ab. Ansonsten wird weiter iteriert,
bis $\norm{d^{(i)}}$ hinreichend klein ist.

Auch im Mehrdimensionalen konvergiert dieses Verfahren lokal
mindestens quadratisch, sofern in einer abgeschlossenen Umgebung der
Nullstelle $\norm{F'(x)^{-1}}$ beschränkt ist und $f$ zweifach stetig
differenzierbar. Allerdings gibt es kein langsames Verfahren ähnlich
der Bisektion, dass man dem Vefahren vorausschicken könnte, um die
Nullstelle einzugrenzen. Die globale Suche nach Nullstellen in
mehreren Dimensionen ist also eine schwierige Aufgabe. Eine
Möglichkeit ist, zunächst mit Hilfe von Optimierungsverfahren ein
$x^{(0)}$ zu finden mit möglichst kleiner Norm $\norm{f(x^{(0)})}$, und von
dort das (gedämpfte) Newtonverfahren zu starten. Die globale
Optimierung in vielen Dimensionen ist selber eine sehr schwierige
Aufgabe, allerdings existieren hierfür Ansätze wie genetische
Algorithmen oder Simulated Annealing, die wir später kennenlernen
werden.

\subsection{Gedämpftes Newtonverfahren}
\index{Newtonverfahren>gedämpftes}

Leider ist im Mehrdimensionalen die Umgebung um die Nullstelle, in der
das Verfahren konvergiert, oftmals deutlich kleiner. Das Verfahren
springt dann leicht über die Nullstelle hinweg, wie es im
Eindimensionalen nur am Anfang des Verfahrens vorkommt
(\zb Abbildung~\ref{fig:newton} rechts, im ersten Schritt). Um dies
zu verhindern, kann man die Schrittweite reduzieren, also den Schritt
$d^{(i)}$ verkürzen. Die Iteration lautet dann
\begin{equation}
  x^{(i+1)} = x^{(i)} + \lambda d^{(i)}\quad\text{mit}\;
  F'(x^{(i)})\,d^{(i)} = -F(x^{(i)}),
\end{equation}
wobei die Dämpfung $\lambda\in (0,1]$ so gewählt wird, dass
$\norm{F(x^{(i+1)})}\le\norm{F(x^{(i)})}$. Dazu wird zum Bespiel mit
$\lambda=1$ begonnen, und $\lambda$ solange verringert, bis die
Bedingung erreicht ist. Der Rechenaufwand für die Iteration ist klein,
da nur die Norm der Funktionswerte berechnet werden muss, was mit
linearem Zeitaufwand möglich ist. Eine Neuberechnung des Schritts
$d^{(i)}$ hingegen erfordert die Lösung eines Gleichungssystems, was
mit der Gaußelimination $\O(N^3)$ Rechenschritte erfordert. Für
vieldimensionale Probleme dominiert dieser Aufwand klar. Eine
Beispielimplementation des gedämpten Newtonverfahrens zeigt
Listing~\ref{lst:newton}.

\begin{lstlisting}[style=floating,float=t,deletekeywords={lambda},
caption={Gedämpftes Newtonverfahren in mehreren
Dimensionen. \lstinline!f(x)! muß
eine vektorwertige Funktion sein, \lstinline!fprime(x)! ihre
Ableitung, d.h. eine matrixwertige Funktion.},label={lst:newton}]
# Gedaempftes Newtonverfahren
#############################

def gedaempfter_newton(f, fprime, x0, epsilon):
   xi = x0
   konvergiert = False
   while not konvergiert:
      # Newton-Korrektur
      di = solve(fprime(xi), f(xi))
      if norm(di) < epsilon:
         konvergiert = True
      else:
         # Schrittweitendaempfung
         lambda = 1.0
         abstieg = False
         while not abstieg:
            # neue Naeherung
            xneu = xi + lambda*di
            if norm(f(xneu)) < norm(f(xi)):
               abstieg = True
            else:
               lambda = lambda / 2.0
         xi = xneu
   return xi
\end{lstlisting}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "padc"
%%% TeX-PDF-mode: t
%%% End: 
