% Dies ist Teil der Vorlesung Physik auf dem Computer, SS 2012,
% Axel Arnold, Universitaet Stuttgart.
% 
% Dieses Werk ist unter einer Creative Commons-Lizenz vom Typ
% Namensnennung-Weitergabe unter gleichen Bedingungen 3.0 Deutschland
% zugänglich. Um eine Kopie dieser Lizenz einzusehen, konsultieren Sie
% http://creativecommons.org/licenses/by-sa/3.0/de/ oder wenden Sie sich
% schriftlich an Creative Commons, 444 Castro Street, Suite 900, Mountain
% View, California, 94041, USA.

\chapter{Optimierung}
\index{Optimierung}

Bei der Optimierung betrachtet man eine Funktion $f:M\to R$, die
\emph{Zielfunktion}, mit einer beliebigen \emph{zulässigen Menge} $M$.
Gesucht ist $x\in M$, so dass $f(x) \le f(x')$ für alle $x'\in M$. Man
schreibt kurz
\begin{equation}
  \label{eq:min}
  \min_{x\in M} f(x).
\end{equation}
Die Menge $M$ ist dabei oft eine Teilmenge des $\RR^n$, die zum
Beispiel durch \emph{Nebenbedingungen} der Form $g(x)\ge 0$
beschrieben wird.

Ein Beispiel einer solchen Aufgabe haben wir bereits im Zusammenhang
mit Funktionsfits kennengelernt. Bei diesem Problem sind Daten $(x_i,
y_i)\in\RR^m\times\RR$, $i=1(1)n$ sowie eine parametrisierte Funktion
$f_v(x)$ gegeben, wobei $v$ freie Parameter sind. Gesucht
wird
\begin{equation}
  \label{eq:fit}
  \min_{v} \sum_{i=1}^n (f_v(x_i) - y_i)^2.
\end{equation}

Im Spezialfall, dass $f_v$ linear ist, also von der Form
$f_{\tilde{v},v_0}(x) = \tilde{v}^Tx + v_0$, ist dies in
Vektorschreibweise äquivalent zur Aufgabe
\begin{equation}
  \label{eq:min2}
  \min_{(\tilde{v},v_0)\in\RR^{m+1}} \sum_{i=1}^n (\tilde{v}^Tx_i -
  v_0 - y_i)^2 = \min_{v\in\RR^{m+1}} \norm{A v - b}_2.
\end{equation}
mit
\begin{equation*}
  b =
  \begin{pmatrix}
    y_1\\
    \vdots\\
    y_n
  \end{pmatrix}
\quad\text{und}\;
A =
\begin{pmatrix}
  (x_1)_1 & \ldots & (x_1)_m & 1\\
  \vdots &        & \vdots & \vdots\\
  (x_n)_1 & \ldots & (x_n)_m & 1\\
\end{pmatrix}.
\end{equation*}

Die klassische lineare Regression benutzt die 2-Norm und versucht
damit, die mittlere Abweichung zu minimieren. Bei bestimmten Aufgaben
ist aber nicht der mittlere, sondern der maximale Fehler
ausschlaggebend. Dies ist zum Beispiel der Fall bei der
Kraftfeldoptimierung für Molekulardynamiksimulationen, bei der eine
Parametrisierung eines Potentials gesucht wird, die bekannte
experimentelle Daten möglichst gut wiedergibt. Offenbar ist die Güte
einer solchen Näherung durch den maximalen Fehler in einer Eigenschaft
bestimmt und nicht durch den durchschnittlichen Fehler.  Aus
\eqref{eq:min2} wird dann
\begin{equation}
  \label{eq:mininf}
  \min_{v\in\RR^{m+1}} \norm{A v -
    b}_\infty = \min_{v\in\RR^{m+1}} \max_{i=1}^n \abs{(A v)_i - y_i}.
\end{equation}
Obwohl sich scheinbar nicht viel geändert hat, hat dieses Problem eine
ganz andere Struktur, für die andere Lösungsmethoden benutzt
werden. Um dies zu verstehen, fügen wir eine weitere Variable
$v_{m+2}$ zum Parameterraum hinzu. Diese soll $v_{m+2} = \norm{A v -
  b}_\infty$ erfüllen, also $v_{m+2}$ minimal mit $-v_{m+2}\le (A v)_i
- y_i \le v_{m+2}$ für $i=1(1)n$. Damit wird aus \eqref{eq:mininf}
eine Minimierungsaufgabe mit linearer Zielfunktion und
Nebenbedingungen:
\begin{equation}
  \label{eq:chebyshevappr}
  \min_v (0,\ldots,\,0,\,1)^T v\quad\text{unter der Bedingung}\;
  \begin{pmatrix}
    A  & e\\
    -A & e
  \end{pmatrix} v \ge
  \begin{pmatrix}
    b\\
    -b
  \end{pmatrix},
\end{equation}
wobei $e=(1,\ldots,1)^T$. Eine solche Aufgabe heißt auch
\emph{\keyword{lineares Programm}} und wird mit Verfahren wie den
Simplexalgorithmus behandelt. Lineare Programme spielen auch in der
Spiele- und Wirtschaftstheorie eine große Rolle.

Die ursprünglich gestellte Aufgabe \eqref{eq:min} oder auch
\eqref{eq:fit} sehen in ihrer allgemeinen Form natürlich auch
nichtlineare Funktionen vor, etwa wenn eine Exponentialfunktion an
Daten angeglichen werden soll oder wenn das Minimum einer komplexeren
Energiefunktion gesucht ist.  In diesem Fall spricht man von
nichtlinearer Optimierung.  Sofern die Zielfunktion zweifach stetig
differenzierbar ist, ist aus der Analysis bekannt, dass im Minimum die
Ableitung verschwindet. Die Nullstellen der Ableitung lassen sich im
Prinzip mit Hilfe des Newtonverfahrens finden, wir lernen aber
bessere, auf die Optimierung zugeschnittene Varianten kennen.

Eine nichtlineare Funktion kann mehrere Minima aufweisen. Ähnlich wie
das Newtonverfahren konvergieren die Verfahren zur nichtlinearen
Optimierung im Allgemeinen nur lokal gegen das nächstgelegene Minimum.
Solche Optimierungsverfahren heißen daher \emph{lokal}. Falls eine
Zielfunktion sehr viele lokale Minima hat, ist es für diese Verfahren
unter Umständen schwierig bis unmöglich, das \emph{globale} Minimum
\eqref{eq:min} zu finden. Dies gilt insbesondere, wenn
Nebenbedingungen gegeben sind, weil sich dann das Minimum auch
irgendwo auf dem Rand der zulässigen Menge befinden kann. Trotzdem
spielen lokale Optimierungsmethoden eine wichtige Rolle, etwa bei der
Energieminimierung. Bei diesem ersten Schritt typischer
Molekulardynamiksimulationen werden die Teilchen zunächst so
verschoben, dass die Energie lokal minimiert wird. Dadurch kann die
eigentliche Simulation mit größeren Zeitschritten begonnen
werden. Eine globale Optimierung ist dabei unnötig, da das System
während der Simulation sowieso nicht im Energieminimum verharrt,
sondern einen hoffentlich ausreichend großen Teil des Phasenraums
besucht.

Die globale Optimierung, also die Suche nach dem kleinsten lokalen
Minimum, ist hingegen vor allem bei der Suche nach Grundzuständen
wichtig. Auch viele praktische Probleme, etwa die Fahrplanoptimierung,
sind globale Optimierungsprobleme. Lokale Verfahren setzen außerdem
die Differenzierbarkeit der Zielfunktion voraus. Ist dies nicht der
Fall oder die zulässige Menge diskret, scheiden lokale Verfahren
ebenfalls aus. Leider gibt es für die allgemeine globale Optimierung
mit nichtlinearen Zielfunktionen oder über diskreten zulässigen Mengen
keine Verfahren mit gesicherten Konvergenzaussagen, wir werden aber
zwei physikalisch bzw.\ biologisch motivierte Verfahren kennenlernen.

\section{Ausgleichsrechnung und \keyword{Pseudoinverse}}
\index{Moore-Penrose-Inverse}

Wir betrachten zunächst lineare Optimierungsprobleme der Form
\begin{equation}
  \label{eq:optnorm2}
  \min_{x\in\RR^n} \norm{Ax-b}_2,
\end{equation}
d.h., wir suchen den Vektor $x$, so dass $Ax$ möglichst nahe an $b$
liegt, wobei $A\in\RR^{m,n}$ und $b\in\RR^{m}$.  Solche Probleme
ergeben sich zum Beispiel bei der Ausgleichsrechnung
\eqref{eq:min2}.

Ist $m = n$ und $A$ regulär, so ist die Lösung unmittelbar klar,
nämlich $A^{-1}b$. Ist aber $m>n$, dann ist das Gleichungssystem
$Ax=b$ im Allgemeinen nicht lösbar; \eqref{eq:optnorm2} besagt dann,
dass wir dasjenige $x$ suchen, dass das Gleichungssystem möglichst gut
löst. Um diese Aufgabe zu lösen, formen wir sie zunächst etwas um:
\begin{equation}
  \norm{Ax - b}^2_2 = (Ax - b)^T(Ax - b)
  = x^TA^TAx - 2b^TAx + b^Tb.
\end{equation}
Es handelt sich also um eine quadratische Optimierungsaufgabe, deren
Minimum $x$
\begin{equation}
  \nabla \norm{Ax - b}^2_2 = \left(\frac{\partial}{\partial x_i} \norm{Ax -
    b}^2_2\right)_i = 2x^TA^TA - 2b^TA = 0
\end{equation}
erfüllt. Hat $A$ linear unabhängige Spalten, so ist $A^TA$ invertierbar,
und wir finden die Lösung als
\begin{equation}
  x = (A^TA)^{-1}A^Tb.
\end{equation}

Ist in \eqref{eq:optnorm2} $m < n$, so ist das Gleichungssystem im
allgemeinen nicht eindeutig zu lösen. Wir können aber die Lösung
minimaler Norm $\norm{x}$ suchen. Dies führt zu der
Optimierungsaufgabe mit Nebenbedingungen
\begin{equation}
  \label{eq:optnorm2bild}
  \min_{x\in\RR^n} \norm{x}_2\quad\text{unter der Bedingung}\; Ax=b.
\end{equation}
Diese Aufgabe können wir ganz ähnlich wie im Falle $m>n$ lösen. Hat
$A$ wenigstens linear unabhängige Zeilen, so ist $AA^T$ invertierbar,
und wir können
\begin{equation}
  x = A^T(AA^T)^{-1}b
\end{equation}
definieren. Dann erfüllt $x$ offenbar die Nebenbedingung $Ax=b$, und
alle anderen zulässigen $x'$ liegen in $x + \text{Kern}(A) = x +
\text{Bild}(A^T)^\perp$. Da $x\in\text{Bild}(A^T)$, gilt
$\norm{x'}=\norm{x} + \norm{x-x'}\ge \norm{x}$. Mit anderen Worten,
$x$ löst die Optimierungsaufgabe \eqref{eq:optnorm2bild}.

Die auftretenden Matrizen $(A^TA)^{-1}A^T$ und $A^T(AA^T)^{-1}$ sind
Spezialfälle der \emph{Pseudoinversen} oder
\emph{Moore-Penrose-Inversen} für allgemeine Matrizen. Der Name
Pseudoinverse rührt daher, dass diese unter den gegebenen
Nebenbedingungen die Gleichungen invertieren, so gut es geht. Die
Pseudoinverse findet also das $x$ mit minimaler Norm unter allen $x$,
für die $\norm{Ax-b}_2$ minimal ist.  Moore und Penrose haben
abstrakte Bedingungen für die Pseudoinverse definiert, die von den
obigen Ausdrücken erfüllt werden. Die Pseudoinverse für beliebige
Matrizen kann durch eine sogenannte Singulärwertzerlegung bestimmt
werden kann. Diese kann in dieser Vorlesung leider nicht behandelt
werden, aber wir werden zumindest sehen, wie bei Matrizen mit
maximalem Zeilen- oder Spaltenrang die Pseudoinverse mit Hilfe
der QR-Zerlegung bestimmt werden kann, ohne $A^TA$ oder $AA^T$
berechnen und invertieren zu müssen. In SciPy berechnet
\scipy{scipy.linalg.pinv(A)} bequem die Moore-Penrose-Pseudoinverse
für eine beliebige Matrix \argd{A}.

Sei also $A\in\RR^{m,n}$ eine Matrix mit maximalem Spaltenrang,
insbesondere $m \ge n$. Aus dem Householder- oder Givensverfahren
erhalten wir
\begin{equation}
  A = 
  \begin{pmatrix}
    Q_1 & Q_2
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    R_1\\
    0
  \end{pmatrix},
\end{equation}
wobei $R_1$ eine reguläre rechte obere $n\times n$-Dreiecksmatrix ist,
und $Q_1$ die zugehörigen $n$ ersten Spalten der unitären Matrix $Q$.
Dann ist
\begin{equation}
  (A^TA)^{-1}A^Tb = (R_1^TQ_1^TQ_1 R_1)^{-1}A^Tb =
  R_1^{-1}\left(R_1^T\right)^{-1}A^Tb,
\end{equation}
was durch bequemes Vorwärts- und Rückwärtseinsetzen ohne
Matrixinversion gelöst werden kann. Wie man sieht, wird in diesem Fall
die Matrix $Q$ nicht benötigt.

Hat umgekehrt $A$ maximalen Zeilenrang, insbesondere also $m \le n$,
dann zerlegen wir $A^T$ mit Hilfe von Householder- oder
Givensverfahren wie oben in eine reguläre rechte obere $m\times
m$-Dreiecksmatrix $R_1$ und orthonormale Matrizen $Q_1\in\RR^{n,m}$
und $Q_2\in\RR^{n,n-m}$. Dann gilt entsprechend
\begin{equation}
  A^T(AA^T)^{-1}b = A^T(R_1^TQ_1^TQ_1 R_1)^{-1}b =
  A^TR_1^{-1}\left(R_1^T\right)^{-1}b.
\end{equation}

Für Zielfunktionen von der Form $\norm{Ax-b}_2$, also gewissermaßen
das Optimierungs-Äquivalent von linearen Gleichungssystemen, lässt
sich also das Optimierungsproblem mit Hilfe der Pseudoinversen
geschlossen lösen.

\section{Nichtlineare Optimierung}
\index{Optimierung>lokale}
\index{Optimierung>nichtlineare}

Für nichtlineare, aber wenigstens zweifach stetig differenzierbare
Zielfunktionen $f:\RR^n\to \RR$ gilt in freien Minima $x$
\begin{equation}
  \label{eq:mingrad}
  \nabla f(x) = 0.
\end{equation}
Dies garantiert allerdings nicht, dass eine gefundene Nullstelle auch
ein Minimum ist, dazu muss zusätzlich noch die Hessesche Matrix
$f''(x)$ positiv definit sein. Selbst in diesem Fall ist der gefundene
Punkt nur ein \emph{lokales} Minimum, also nur in einer kleinen
Umgebung sind alle anderen Punkte höher. Eine Funktion kann aber
natürlich ohne weiteres mehrere Minima haben, die sich nur durch
erneute Suche finden lassen. Insbesondere ist es praktisch unmöglich,
das \emph{globale} Minimum zu finden; damit werden wir uns später noch
einmal beschäftigen.

Für \eqref{eq:mingrad} können wir das mehrdimensionale Newtonverfahren
formulieren. Wir wählen also einen Startpunkt $x^{(0)}$ in der Nähe
des Minimums und setzen
\begin{equation}
  x^{(k+1)} = x^{(k)} - f''\left(x^{(k)}\right)^{-1}\nabla f\left(x^{(k)}\right),
\end{equation}
wobei
\begin{equation}
  f''(x) = 
  \left(\frac{\partial^2}{\partial x_j\partial x_k}f(x)\right)_{k,j} = 
  \begin{pmatrix}
    \frac{\partial^2}{\partial x_1^2}f(x) & \ldots &
    \frac{\partial}{\partial x_1\partial x_n}f(x)\\
    \vdots               &        & \vdots \\
    \frac{\partial}{\partial x_1\partial x_n}f(x) & \ldots &
    \frac{\partial^2}{\partial x_1^2}f(x)
  \end{pmatrix}.
\end{equation}
Bei großen Dimensionen $n$ kann es rasch sehr aufwendig werden,
$f''(x)$ zu berechnen, dies ist aber auch nötig, um zu überprüfen,
ob das gefundene Extremum auch tatsächlich ein Minimum ist. Daher
ist dieses Verfahren nicht optimal. Besser sind die folgenden
Verfahren, die ohne $f''(x)$ auskommen.

\subsection{\keyword{Verfahren des steilsten Abstiegs}}
\index{Gradientenabstiegsverfahren}

Da das Newtonverfahren, das wir oben auf die Ableitung angewendet
haben, auf einer Taylorentwicklung erster Ordnung
basiert, basiert die Optimierung mit Hilfe des Newtonverfahrens in
gewisser Weise auf einer Taylorentwicklung zweiter Ordnung. Was können
wir nun mit der praktikableren Taylorentwicklung erster Ordnung
erreichen? Diese ist zunächst einmal
\begin{equation}
  \label{eq:steepestdescentexpand}
  f(x + \lambda d) = f(x) + \lambda \nabla f(x)^Td + \O(\lambda^2)
\end{equation}
für eine Richtung $d$ und \emph{Schrittweite} $\lambda > 0$.  Anders
als beim Newtonverfahren können wir nun nicht das Minimum dieser
Näherung als neue Iterierte benutzen, da die Näherung linear ist und
daher kein Minimum hat. Daher können wir lediglich versuchen, $f$ zu
verringern. Da $\lambda>0$ ist und wir für kleine $\lambda$ die
quadratischen Anteile vernachlässigen können, muss $\nabla f(x)^Td < 0$
gelten. Eine Richtung $d$, die dies erfüllt, heißt
\emph{Abstiegsrichtung}.

Den maximalen Abstieg erreichen wir, wenn $d = -\nabla f(x)$; diese
Richtung heißt daher auch steilster Abstieg. Für das \emph{Verfahren
  des steilsten Abstiegs} (auch \emph{Gradientenabstiegsverfahren}
wählt man zunächst einen Startwert $x^{(0)}$ und setzt dann
\begin{equation}
  \label{eq:steepestdescent}
  x^{(k+1)} = x^{(k)} - \lambda \nabla f\left(x^{(k)}\right)
\end{equation}
mit einer geeigneten Schrittweite $\lambda>0$. Im einfachsten Falle
ist $\lambda$ eine kleine Konstante, zum Beispiel $0,01$.

\subsection{\keyword{Schrittweitensteuerung}}
\index{Armijo-Schrittweite}

Besser ist aber, die Schrittweite so zu wählen, dass das Verfahren
sicher absteigt. Dafür gibt es verschiedene Verfahren, von denen hier
nur die recht effizienten \emph{Armijo-Schrittweiten} besprochen
werden.

Entlang der festgelegten Richtung $d$ ist die Optimierung nur noch ein
eindimensionales Problem, und wegen \eqref{eq:steepestdescentexpand}
gibt es für alle $\alpha<\nicefrac{1}{2}$ eine kleine Umgebung von
$x$, in der
\begin{equation}
  \label{eq:armijo}
  f(x + \lambda d) \le f(x) + \alpha\lambda \nabla f(x)^Td
\end{equation}
gilt. Wir wählen $\lambda$ so, dass diese Bedingung erfüllt ist, und
natürlich möglichst groß. $\alpha\in(0,\,\nicefrac{1}{2})$ bestimmt
dabei den Mindestabstieg, den wir erreichen wollen. Um $\lambda$ zu
bestimmen, beginnen wir einfach mit $\lambda_0=1$, und setzen
anschließend $\lambda_{k+1} = \rho\lambda_{k}$, solange
\eqref{eq:armijo} nicht erfüllt ist. $\rho\in (0,1)$ ist dabei eine
weiter Verfahrenskonstante, die bestimmt, wie rasch wir $\lambda$
verkleinern. Um die Bedingung zu überprüfen, benötigen wir lediglich
die beiden reellen Konstanten $\nabla f(x)^Td$ und $f(x)$ und müssen
in jedem Schritt $f(x + \lambda d)$ neu auswerten.  $\alpha$ wird
meist eher klein gewählt, etwa $0,1$, denn je strikter diese
Bedingung, desto kleiner wird die Schrittweite. Umgekehrt sollte man
$\rho$ nicht zu klein wählen, weil sonst unnötig kleine
Schrittweiten benutzt werden.

Die Schrittweitensteuerung setzt nur eine Abstiegsrichtung $d$ voraus,
und kann daher zum Beispiel auch auf das Newton-Verfahren angewandt
werden, dessen Richtung $d=-f''\left(x^{(k)}\right)^{-1}\nabla
f\left(x^{(k)}\right)$ ist. Diese Richtung ist dann eine
Abstiegsrichtung, wenn $\nabla f\left(x^{(k)}\right)
f''\left(x^{(k)}\right)^{-1} \nabla f\left(x^{(k)}\right)$, also, wenn
$f''\left(x^{(k)}\right)^{-1}$ positiv definit ist. Dies ist zumindest
in einer Umgebung eines Minimums der Fall.

\subsubsection{Beispiel: Gradientenabstiegsverfahren mit
  Schrittweitensteuerung}
\label{sec:armijosd}

In Python sieht das Verfahren des steilsten Abstiegs mit
Armijo-Schrittweiten so aus:
\lstinputlisting[firstline=10]{armijo.py}%
Die Funktion \lstinline!armijo_steepest_descent! benötigt dabei als
Eingabeparameter Pythonfunktionen \argd{f} und \argd{gradf}, die die
zu minimierende Funktion und ihren Gradienten zurückliefern. Die
Konstanten \argd{alpha} und \argd{rho} entsprechen den Konstanten
$\alpha$ und $\rho$ des Armijo-Verfahrens. Die Vorgaben $\alpha=0,1$
und $\rho=0,5$ sind übliche Werte, die meist zu guter Konvergenz
führen. Das Verfahren bricht ab, wenn $\norm{\nabla f(x^{(k)})}<$
\argd{tol}, also die Tangente hinreichend flach und damit ein Minimum
hinreichend gut gefunden ist, oder wenn eine maximale Anzahl an
Schritten, \argd{maxiter}, überschritten wird. Dies vermeidet ein
Einfrieren durch zu kleine Schrittweiten. Dies passiert insbesondere,
wenn der Funktionswert im Minimum sehr groß oder klein ist, so dass
nur wenige Stellen zur Optimierung verbleiben.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/steepestdescent}
  \caption{Verfahren des steilsten Abstiegs für eine quadratische
    Funktion (oben) und die Rosenbrockfunktion (unten). Links werden
    Armijo-Schrittweiten mit $\alpha=0,1$ und $\rho=0,5$, rechts eine
    konstante Schrittweite von $0,01$ verwendet. Die roten Punkte
    markieren die Iterierten $x^{(k)}$, die Höhenlinien illustrieren
    die zu optimierende Funktion. Für die quadratische Funktion
    konvergieren beide Verfahren gegen das Minimum bei $(0,\,0)$, und
    auch das Armijo-Verfahren wählt eine konstante Schrittweite von
    etwa $0,0078$. Durch diesen etwas kleineren Wert zielt das das
    schrittweitengesteuerte Verfahren weniger über das Minimum hinaus
    und braucht nur 11 statt 35 Schritte für 10 Stellen
    Genauigkeit. Bei der Rosenbrockfunktion benötigt das
    schrittweitengesteuerte Verfahren etwa 1700 Iterationen dafür.
    Jeder 200. Punkt ist im Graphen grau gefärbt, um die extrem
    langsame Konvergenz zu visualisieren. Für die konstante
    Schrittweite $0,01$ konvergiert das Verfahren gar nicht, wie die
    eingezeichneten ersten zehn Punkte zeigen.} \label{fig:armijo}
\end{figure}

Als Beispiel betrachten wir das Verfahren des steilsten Abstiegs für
zwei auf der Ebene definierte Funktionen. Die erste Funktion ist eine
quadratische Funktion
\begin{equation}
  \label{eq:quadgl}
  f(x, y) = 40 x^2 + 30(x + y)^2 + 20 y^2,
\end{equation}
die ihr Minimum bei $(0,0)$ hat, die zweite Funktion ist die
Rosenbrockfunktion\index{Rosenbrockfunktion}
\begin{equation}
  f_{\text{Rosenbrock}}(x, y) = (1-x)^2 + 100(y-x^2)^2.
\end{equation}
Diese hat ihr Minimum offenbar bei $(1,1)$, ist aber das
Optimierungsäquivalent zur Rungefunktion. Denn während das Minimum in
einem sehr steilen Tal liegt, ist der Gradient entlang der Talsohle
sehr flach. Die meist gierigen Optimierungsverfahren finden daher sehr
schnell in die Talsohle, kommen dort aber nur noch langsam ins Ziel,
da die Hauptabstiegsrichtung vorwiegend in die Talsohle statt
entlang dieser zeigt.

Abbildung~\ref{fig:armijo} zeigt die Anwendung des Verfahrens des
steilsten Abstiegs mit und ohne Schrittweitensteuerung auf die beiden
Funktionen und illustriert dabei die beiden Hauptschwächen des
Gradientenabstiegsverfahrens. Einerseits neigt es dazu, über das
Minimum hinauszuschießen, und sich dadurch langsam in das Minimum
einzupendeln. Dies lässt sich durch die Schrittweitensteuerung
begrenzen. Andererseits muss der steilste Abstieg nicht in Richtung
des Minimums zeigen, in diesem Fall neigt das Verfahren des steilsten
Abstiegs dazu, sich dem Minimum in Zick-Zack-Kurven mit sehr geringer
Schrittweite zu nähern.

Bei zu großer Schrittweite kann das Verfahren sogar gar nicht
konvergieren. Da von vornherein die maximale Ableitung meist nicht
bekannt ist, ist also eine Schrittweitensteuerung unerlässlich.

\subsection{\keyword{CG-Verfahren}}
\index{konjugierter Gradient}

Wir betrachten nun einen wichtigen Spezialfall, nämlich quadratische
Funktionen der Form
\begin{equation}
  \label{eq:cgfunktion}
  f(x) = \frac{1}{2}x^TAx - x^Tb
\end{equation}
mit positiv definiter Matrix $A\in\RR^{n,n}$.  Dies bedeutet, dass $A$
symmetrisch ist und $x^TAx>0$ für alle Vektoren $x\neq 0$ erfüllt,
etwa, weil alle Eigenwerte positiv sind.  Positiv definite Matrizen
treten zum Beispiel bei quantenmechanischen Rechnungen auf.

Für Funktionen der Form \eqref{eq:cgfunktion} gibt es ein iteratives
Verfahren, dass bei unbegrenzter Genauigkeit in spätestens $n$
Schritten gegen die exakte Lösung konvergiert. Dieses Verfahren ist
das konjugierte Gradienten-Verfahren (englisch conjugate gradient,
daher kurz auch CG-Verfahren).
Das Optimierungsproblem $\min f(x)$ kann dabei auch als Gleichung
aufgefasst werden, da
\begin{equation}
  Ax = b \quad\iff\quad x\;\text{minimiert}\;f(x) = \frac{1}{2}x^TAx - x^Tb.
\end{equation}
Daher wird das CG-Verfahren auch als effizienter
Gleichungslöser für positiv definite Matrizen behandelt.

Um das Minimum iterativ zu suchen, starten wir mit einem beliebigen
Startwert $x^{(0)}$ (zum Beispiel 0), und verfeinern die aktuelle
Näherung gemäß $x^{(k+1)} = x^{(k)} + \lambda^{(k)}d^{(k)}$ so, dass
wir uns dem Minimum nähern. Wie wir gesehen haben, ist der steilste
Abstieg, also in Richtung des negierten Gradienten $r^{(k)} =
b-Ax^{(k)}$, nicht ideal. Die Idee des CG-Verfahrens ist nun, die
Richtungen $d^{(k)}$ in gewisser Weise stets senkrecht zu einander zu
wählen, so dass Hin- und Herpendeln oder Zick-Zack-Kurse
ausgeschlossen sind. Tatsächlich wählt man die Abstiegsrichtungen
senkrecht in dem durch $A$ induzierten Skalarprodukt, also so, dass
$(d^{(i)}, d^{(k)})_A := \left(d^{(i)}\right)^TAd^{(k)}=0$ für $i\neq
k$. Man sagt auch, dass $d^{(i)}$ und $d^{(k)}$ $A$-konjugiert sind,
daher der Name des Verfahrens.

\begin{figure}
  \centering
  \includegraphics[width=0.5\textwidth]{plots/cg}
  \caption{CG-Verfahren für die quadratische Funktion aus
    \eqref{eq:quadgl}. Die Höhenlinien stellen die zu optimierende
    quadratische Funktion dar, die roten Punkte markieren die
    Iterierten des CG-Verfahrens, das in nur 2 Schritten konvergiert.}
  \label{fig:cg}
\end{figure}

Die $d^{(k)}$ werden konstruiert, indem die
Gradientenabstiegsrichtungen $r^{(k)}$ mit Hilfe des
Gram-Schmidt-Verfahrens zueinander senkrecht bezüglich
$(\cdot,\cdot)_A$ gemacht werden. Dabei muss aufgrund der Wahl der
Richtungen $d^{(k+1)}$ nur senkrecht zu $d^{(k)}$ gemacht werden, zu
den anderen bisherigen Richtungen steht $d^{(k+1)}$ dann automatisch
senkrecht. Der Vorfaktor kann dabei vereinfacht werden:
\begin{equation}
  \frac{(r^{(k+1)}, d^{(k)})_A}{(d^{(k)}, d^{(k)})_A}
  = -\frac{\left(r^{(k+1)}\right)^Tr^{(k+1)}}{\left(r^{(k)}\right)^Tr^{(k)}}.
\end{equation}

Die Schrittweite kann bei einer quadratischen Funktion so bestimmt
werden, das $\lambda$ die Funktion entlang der Richtung $d$, also
\begin{equation}
  \frac{1}{2}(x+\lambda d)^TA(x+\lambda d) - b^T(x+\lambda d)
  =  \frac{1}{2}x^TAx - b^Tx + \lambda d^T(Ax - b) + \frac{1}{2}\lambda^2 d^TAd,
\end{equation}
minimiert. Es ergibt sich
\begin{equation}
  \lambda^{(k)}  =  \frac{\left(d^{(k)}\right)^Tr^{(k)}}{\left(d^{(k)}\right)^TAd^{(k)}}.
\end{equation}

Zusammengefasst ergibt sich das CG-Verfahren in Python als
\lstinputlisting[firstline=10]{cg.py}%
Das Verfahren bricht ab, wenn $\norm{r^{(k)}} <$ \argd{tol}, anstatt
stur $n$ Schritte zu berechnen, da durch numerische Ungenauigkeiten
unter Umständen 1-2 zusätzlich Schritte nötig werden können. Umgekehrt
können natürlich auch weniger Schritte notwendig sein, wenn der
Startwert günstig liegt.

Abbildung~\ref{fig:cg} illustriert das CG-Verfahren an der selben
quadratischen Funktion, für die das schrittweitengesteuerte
Gradientenabstiegsverfahren 11 Schritte brauchte, um 10 Stellen
Genauigkeit zu erreichen. Das CG-Verfahren hingegen konvergiert in nur
zwei Schritten auf Maschinengenauigkeit, also etwa 17 Stellen.

Da das Verfahren auch als Gleichungslöser sehr gute Eigenschaften hat
und wegen der überwiegenden Skalar- und Matrix-Vektorprodukte auch
sehr einfach auf dünnbesetzten Matrizen eingesetzt werden kann, ist es
eines der meist genutzten Verfahren. Daher gibt es natürlich auch eine
SciPy-Implementation, \scipy{scipy.sparse.linalg.cg(A, b)}.

\subsection{Nebenbedingungen und Straffunktionen}
\index{Straffunktion}
\index{Penalty function}

Mit dem Verfahren des steilsten Abstiegs und den Armijo-Schrittweiten
haben wir ein stabiles und meist schnell konvergierendes Verfahren, um
lokal freie Minima zu suchen. Was aber kann man tun, wenn zusätzlich
noch Nebenbedingungen gegeben sind? Wir suchen nun also $\min_{x\in M}
f(x)$, wobei die zulässige Menge
\begin{equation}
  M=\left\{ x | g_i(x)\ge 0,\,i=1(1)m \right\}
\end{equation}
ist. Bekannte Verfahren sind etwa die sequenzielle quadratische
Programmierung (SQP) oder das Verfahren von Gill und
Murray~\cite{gill78a}. Hier lernen wir einen anderen, physikalisch
motivierten Ansatz kennen.

Dazu setzen wir voraus, dass nicht nur die zu minimierende Funktion
$f:\RR^n\to\RR$ stetig differenzierbar ist, sondern auch die
Funktionen $g_i:\RR^n\to\RR$, die die Nebenbedingungen
definieren. Wäre nun $g_i(x)=-\infty$ für alle $x\notin M$ oder
zumindest kleiner als das gesuchte Minimum von $f$ in $M$, so wäre
\begin{equation}
  \min_{x\in M} f(x) = \min_{x\in\RR^n} f(x) + \sum_{i=1}^m \min(0,
  g_i(x))^2.
\end{equation}
In der Praxis ist einerseits $g_i(x)$ im Allgemeinen endlich,
andererseits kennen wir aber auch kein Verfahren, um eine höchst
unstetige Funktion zu minimieren. Wenn wir uns die Nebenbedingungen
aber als Banden vorstellen, über die ein iterativer Algorithmus nicht
hinausschreiten darf, könnte man diese zunächst weicher gestalten, so
dass der Algorithmus den zulässigen Bereich etwas verlassen kann, und
dann die Banden mit der Zeit immer härter gestalten, so dass der
Algorithmus schließlich in den zulässigen Bereich gedrückt
wird. Befindet sich das Minimum über $M$ am Rand von $M$, wird die
gefundene Näherungslösung daher immer minimal außerhalb von $M$
liegen. Ist dies ein Problem, etwa weil $f$ außerhalb $M$ nicht
ausgewertet werden kann, kann man stattdessen auf Barrieremethoden
ausweichen, bei denen die Banden bereits innerhalb von $M$ beginnen
und am Rand von $M$ singulär sind.

Beim Straffunktionsverfahren minimieren wir also die modifizierte
Funktion
\begin{equation}
  \label{eq:penalty}
  q_{\sigma}(x) = f(x) + \sum_{i=1}^m \min(0,
  \sigma g_i(x))^2.
\end{equation}
Der hintere Teil $p_{\sigma}(x) = \sum_{i=1}^m \min(0, \sigma
g_i(x))^2$ wird dabei Straffunktion (Penalty function)
genannt, weil er Punkte außerhalb des zulässigen Bereichs mit höheren
Funktionswerten bestraft.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/penalty}
  \caption{Straffunktionsverfahren für einen Halbkreis. Die
    durchgezogenen, gestrichelten, und gepunkteten Linien markieren
    die Isolinien $p_{\sigma}(x)=\nicefrac{1}{2}$ in den Schritten 1,2
    und 5. Minimiert wird jeweils der Abstand zum den mit offenen
    Symbolen markierten drei Punkten. Der rote Kreis liegt im inneren,
    das Verfahren braucht nur einen Aufruf des freien Optimierers. Für
    die grünen und blauen Dreiecke sind jeweils mehrere Aufrufe nötig,
    wobei das blaue, nach unten zeigende Dreieck so gewählt ist, dass
    das Minimum in der Ecke liegt. Im rechten Graphen sind die
    Straffunktionen entlang der $y$-Achse, also $p_{\sigma}(0,y)$, für
    dieselben Schritte gezeigt. Dünner grün sind die korrespondieren
    Zielfunktionen $q_{\sigma}(0,y)$ eingezeichnet.}
  \label{fig:penalty}
\end{figure}

Um das Minimum zu finden, starten wir mit recht kleinem
$\sigma_0>0$. Auf $q_{\sigma_0}$ wenden wir dann ein
Optimierungsverfahren für freie Optimierung an, etwa das Verfahren des
steilsten Abstiegs mit Schrittweitensteuerung. Aufgrund der
Konstruktion ist der Gradient von $q_{\sigma}$ recht einfach zu berechnen:
\begin{equation}
  \nabla q_{\sigma}(x) = \nabla f(x) + \sum_{i=1}^m 2\sigma^2\min(0,
  g_i(x))\nabla g_i(x).
\end{equation}

Sinnvollerweise wählt man einen Startpunkt im Inneren von $M$. Ist im
gefundenen, freien Minimum von $q$ bereits $g_i(x)\ge - \tau$ mit
gewünschter Toleranz $\tau$ erfüllt, ist das Minimum gefunden, und
liegt sehr wahrscheinlich im Inneren vom $M$. Ansonsten wird
$\sigma_k$ erhöht und erneut die Minimierung gestartet, allerdings
naheliegenderweise mit dem bereits gefundenen Minimum als
Startwert. Die Lage des Minimums ändert sich dabei, da ja wegen der
Abbruchbedingung wenigstens eine Nebenbedingung noch nicht erfüllt
war, und sich diese durch die Änderung von $\sigma_k$ ändert. Solange
danach $g_i(x)\ge - \tau$ nicht erreicht ist, passen wir $\sigma_k$ an
und iterieren weiter.
$\sigma_k$ kann auf verschiedene Weisen gewählt werden, es muss nur
$\sigma_k\to \infty$ für $k\to\infty$ gelten. Eine Möglichkeit ist
etwa $\sigma_k=k^a$ , wobei man $a>1$ nicht zu groß wählen sollte, zum
Beispiel $a=2$.

Abbildung~\ref{fig:penalty} illustriert das Verfahren am Beispiel
einer Abstandsminimierung zu einem Halbkreis. Wir suchen also
\begin{equation}
  \min_{x\in M} \norm{x - p}_2\quad\text{mit}\;
  M = \{ (x,y)\,|\, g_i(x,y) \ge 0, i=1,2 \}
\end{equation}
mit
\begin{equation*}
  g_1(x, y) = 4 - x^2-y^2\quad\text{und}\; g_2(x,y) = -10y.
\end{equation*}
Dies entspricht offenbar $\norm{(x,y)}_2\le 2$ und $y<0$, also dem
Halbkreis. Im Beispiel werden drei verschiedene Punkte betrachtet und
mit Hilfe des Straffunktionsverfahrens und einem
schrittweitengesteuerten Gradientenverfahren der nächste Punkt im
Halbkreis gesucht. Der Straffaktor $\sigma$ wurde als $0,1\,k^2$
gewählt, mit $k=1,2,\ldots$ der Iterationsschritt. $p=(-1,\,-1)$ liegt
selber im Inneren des Halbkreises und wird daher gleich im ersten
Optimierungsschritt als nächster Punkt gefunden. Für die beiden
anderen Punkte, $p=(-1,\,1)$ und $p=(2,5,\,1)$, sind nur die Gerade
beziehungsweise beide Ungleichungen gleichzeitig aktiv; auch hier
werden die nächsten Punkte am Rand des Halbkreises innerhalb weniger
Iterationen gefunden. Auf der rechten Seite sieht man, dass die
Straffunktionen sehr steil werden. Daher ist eine
Schrittweitensteuerung hier besonders wichtig.

\section{Lineare Programme und Simplexalgorithmus}
\index{lineares Programm}
\index{Simplexalgorithmus}

Wie wir in der Einleitung gesehen hatten, führt die lineare Regression
mit Maximumsnorm zu einem ganz anderen Typ von Problem, bei dem die
Zielfunktion linear ist. Dadurch liegt das Minimum notwendigerweise
auf dem Rand der zulässigen Menge. Sind zusätzlich noch die
Nebenbedingungs(un-)gleichungen linear
(vgl. Abbildung~\ref{fig:simplex}), spricht man von einem
\emph{linearen Programm}. Programm hat hier also erst einmal nichts
mit Computern zu tun. Solche linearen Programme spielen auch in der
Optimierung von Wirtschaftsprozessen eine wichtige Rolle ("`Operations
research"'), da Gewinn und produzierte Mengen bei unverändertem Preis
linear von einander abhängen.

\begin{figure}
  \centering
  \begin{tikzpicture}[x=8em,y=8em]
    \draw[->] (-0.1,0) -- (1.2,0);
    \draw[->] (0,-0.1) -- (0,1.2);

    % zulässiger Bereich
    \draw (1,0) -- (0,1) ;
    \fill[pattern=north east lines,rotate around={135:(1,0)}] (1,0) rectangle +(1.414,-0.1) ;
    \fill[pattern=north east lines] (0,0) rectangle +(1,-0.1) ;
    \fill[pattern=north east lines] (0,0) rectangle +(-0.1,1) ;

    % Zielfunktion
    \pgfmathsetmacro{\gradx}{-0.3}
    \pgfmathsetmacro{\grady}{ 0.4}
    \pgfmathsetmacro{\perpx}{\grady}
    \pgfmathsetmacro{\perpy}{-\gradx}

    \draw[very thick,->=triangle 45] (0.5,0.2) -- +(\gradx,\grady);

    % Isolinien
    \foreach \l in {0,0.2,0.4,...,0.8} {
      \pgfmathsetmacro{\ll}{(1-\l)/(\perpx+\perpy)}     
      \draw (0, \l) -- (\ll*\perpx,\l+\ll*\perpy);
    }

    \foreach \l in {0.2,0.2,0.4,...,0.8} {
      \pgfmathsetmacro{\ll}{(1-\l)/(\perpx+\perpy)}     
      \draw (\l, 0) -- (\l + \ll*\perpx, \ll*\perpy);
    }

    % Minimum
    \filldraw (1, 0) circle (0.04);

  \end{tikzpicture}
  \hspace{4em}
  \begin{tikzpicture}[x={(4em,2em)},y={(4em,-2em)},z={(0em,5em)}]
    \draw (xyz cs:x=-.1) -- (xyz cs:x=0);
    \draw (xyz cs:y=-.1) -- (xyz cs:y=0);
    \draw (xyz cs:z=-.1) -- (xyz cs:z=0);

    \draw[thick,->] (xyz cs:x=0) -- (xyz cs:x=1.3) node[right] {$x$};
    \draw[thick,->] (xyz cs:y=0) -- (xyz cs:y=1.3) node[right] {$y$};
    \draw[thick,->] (xyz cs:z=0) -- (xyz cs:z=1.3) node[above] {$z$};

    \draw[color=green,fill=green!80!white,opacity=0.5]
    (xyz cs:z=1) -- (xyz cs:x=1) -- (xyz cs:y=1) -- cycle;
    \draw[color=green]
    (xyz cs:z=1) -- (xyz cs:x=1) -- (xyz cs:y=1) -- cycle;

    \draw[color=green]
    (xyz cs:z=1) circle (0.02)
    (xyz cs:x=1) circle (0.02)
    (xyz cs:y=1) circle (0.02);

    \draw[color=red]
    (xyz cs:z=1,x=0.5) -- (xyz cs:z=0.2,y=0.8);

    \draw[color=red]
    (xyz cs:z=1,x=0.5)   circle (0.02)
    (xyz cs:z=0.2,y=0.8) circle (0.02);
  \end{tikzpicture}
  \caption{Links: Illustration eines linearen Programms. Die
    Nebenbedingungen sind $x\ge 0$, $y\ge 0$ und $1 - x \ge y$; die
    Schraffuren markieren den ausgeschlossenen Bereich. Der Pfeil
    deutet den Gradienten der Zielfunktion an, die Linien im
    zulässigen Bereich sind Isolinien des Potentials. Hier findet sich
    das Minimum in der mit einem Punkt markierten rechten unteren
    Ecke. Rechts: Dreidimensionaler zulässiger Bereich $x\ge 0$ mit
    weiterer Nebenbedingung. Ist eine Nebenbedingung $a^Tx=b$ gegeben,
    so sind nur Punkte mit Koordinaten $\ge 0$ aus dieser Ebene
    zulässig, also aus dem grünen Dreieck. Die Projektion dieses
    Dreiecks in die $xy$-Ebene entspricht dem zulässigen Bereich in
    der linken Illustration. Die Ecken lassen sich durch die Menge der
    Koordinaten, die nicht Null sind, vollständig beschreiben. Die
    unterste Ecke etwa hat nur $y$ aktiv, die obere $z$. Sind zwei
    Nebenbedingungen gegeben, so ist die resultierende zulässige Menge
    eine Gerade (hier rot eingezeichnet). Die Ecken werden nun durch
    zwei aktive Koordinaten beschrieben, hier $y$ und $z$ für die
    vordere Ecke, und $x$ und $z$ für die hintere.}
  \label{fig:simplex}
\end{figure}

Lineare Programme können in einer Vielzahl von Formen definiert
werden, wir betrachten hier die Normalform
\begin{equation}
  \label{eq:simplexprob}
  \min c^Tx \quad\text{unter den Nebenbedingungen}\; x\ge 0,\;Ax=b
\end{equation}
mit $c\in\RR^n$, $A\in\RR^{m,n}$ und $b\in\RR^{m}$. Die Anzahl $m$ der
Gleichungsbedingungen ist dabei nicht festgelegt. Da $x$ ein Vektor
ist, bedeutet $x\ge 0$ einfach, dass alle Komponenten größer als Null
sein sollen. Alle linearen Nebenbedingungen lassen sich so
formulieren, eventuell unter Zuhilfenahme von weiteren Variablen.

Betrachten wir zwei Beispiele. Die in der Abbildung~\ref{fig:simplex}
gegebene Nebenbedingung $1 - x \ge y$ ist nicht von der obigen
Form. Wir führen deswegen eine neue Variable $z=1-x-y$ ein, die dann
$z\ge 0$ erfüllen muss. Wir erhalten die Normalform mit $c'=(c_0, c_1,
0)$, da $z$ ja nicht zur Zielfunktion beiträgt, sowie $A=(1,\,1,\,1)$
und $b=1$. Abbildung~\ref{fig:simplex} rechts zeigt den resultierenden
zulässigen Bereich im dreidimensionalen Raum.

Auch das Problem
\begin{equation}
  \min_v (0,\ldots,\,0,\,1)^T v\quad\text{unter der Bedingung}\;
  \begin{pmatrix}
    A & e\\
    -A & e
  \end{pmatrix} v \ge
  \begin{pmatrix}
    b\\
    -b
  \end{pmatrix},
\end{equation}
das sich bei der linearen Regression mit Maximumsnorm ergibt
(vgl.~\eqref{eq:chebyshevappr}), ist nicht in Normalform. Zum einen
müssen wir die Nebenbedingungsungleichungen in Gleichungen
transformieren. Dazu führen wir einfach eine Schattenvariable
$z_i=a_i^Tv - b_i$ pro Ungleichung $a_i^Tv\ge b_i$ ein. Dann ist
$z_i\ge 0$ offenbar äquivalent zur ursprünglichen Ungleichung. Zum
anderen sind aber die Variablen $v$ frei, während wir voraussetzen,
dass $v\ge 0$. Um dies zu beheben, teilen wir jede Variable in $v=v_+
- v_-$ mit $v_\pm\ge 0$ auf und ergänzen $A$ und $c$ entsprechend. Das
ergibt die äquivalente Aufgabe%
{\samepage\vspace{0.2em}% für die TikZ-Deko
  \begin{equation}
    \label{eq:chebyshevapprnf}
    \min_x (\tikzlabel{vpl}0,\ldots,\,0,\,1\tikzlabel{vpr},
    \tikzlabel{vml}0,\ldots,\,0,\,-1\tikzlabel{vmr},
    \tikzlabel{zl}0,\ldots,\,0\tikzlabel{zr})^T v
  \end{equation}
  \begin{tikzpicture}[overlay, remember picture]
    \draw[decorate,decoration=brace] ($(vpl) + (0,1em)$) --
    node[above] {$v_+$} ($(vpr) + (0,1em)$);
    \draw[decorate,decoration=brace] ($(vml) + (0,1em)$) --
    node[above] {$v_-$} ($(vmr) + (0,1em)$);
    \draw[decorate,decoration=brace] ($(zl) + (0,1em)$) --
    node[above] {$z$} ($(zr) + (0,1em)$);
  \end{tikzpicture}}
mit der neuen Variablen $x=(v_+,v_-,z)$ und den Nebenbedingungen $x\ge 0$
und
{\samepage\vspace{0.2em}% für die TikZ-Deko
\begin{equation*}
  \begin{pmatrix}
    \tikzlabel{vpl}A  & e\tikzlabel{vpr} & 
    \tikzlabel{vml}-A & -e\tikzlabel{vmr} &
    \tikzlabel{zl} -I & 0\tikzlabel{zr}\\
    -A & e &  A & -e & 0 & -I
  \end{pmatrix}
  x =
  \begin{pmatrix}
    b\\
    -b
  \end{pmatrix}.
\end{equation*}
\begin{tikzpicture}[overlay, remember picture]
  \draw[decorate,decoration=brace] ($(vpl) + (0,1em)$) --
  node[above] {$v_+$} ($(vpr) + (0,1em)$);
  \draw[decorate,decoration=brace] ($(vml) + (0,1em)$) --
  node[above] {$v_-$} ($(vmr) + (0,1em)$);
  \draw[decorate,decoration=brace] ($(zl) + (0,1em)$) --
  node[above] {$z$} ($(zr) + (0,1em)$);
\end{tikzpicture}}

Wie können wir Aufgabe \eqref{eq:simplexprob} lösen? Sofern die
zulässige Menge beschränkt ist, beschreiben die Nebenbedingungen immer
einen Polyeder. Man kann nun zeigen, dass sich ein Optimum immer in
einer der Ecken dieses Polyeders befindet. Der
\emph{Simplexalgorithmus} besucht nacheinander die Ecken des
Polyeders, wobei die Zielfunktion ständig kleiner wird. Da es nur
endliche viele Ecken gibt, findet der Algorithmus das Minimum
irgendwann. Es gibt pathologische Beispiele, in denen der Algorithmus
alle Ecken absuchen muss. Im Allgemeinen konvergiert der
Simplexalgorithmus aber schnell.

Die Methode zerfällt dabei in zwei Phasen. In der ersten Phase muss
eine gültige Ecke bestimmt werden, in der zweiten Phase müssen wir
dann von einer gültigen Ecke aus eine neue, benachbarte Ecke mit
niedrigerer Zielfunktion finden. Wir beginnen mit der Beschreibung der
zweiten Phase, da die die erste Phase die zweite Phase benutzt, um ein
erweitertes Problem zu lösen, dass erlaubt, eine erste Ecke zu finden.

\subsubsection*{Phase II}

Wir nehmen an, dass die Matrix $A$ maximalen Zeilenrang hat, also alle
doppelten Gleichung eliminiert wurden; dies wird später die erste
Phase übernehmen.  Außerdem sei $x$ eine Ecke des Polyeders.

Wie stellen wir diese Ecke dar? Abbildung~\ref{fig:simplex}
illustriert, dass eine Ecke einfach durch die Menge der aktiven, also
von Null verschiedenen Koordinaten beschrieben werden kann. Da wir
angenommen haben, dass $A\in\RR^{m,n}$ linear unabhängige Zeilen hat,
beschreibt $Ax=b$ einen $n-m$-dimensionalen Unterraum, dessen Ecken
jeweils $m$ aktive Koordinaten haben.

Sei $B=\{j_1,\ldots,k_m\}$ die Menge der in der aktuellen Ecke aktiven
Koordinaten, die sogenannten Basiskoordinaten, und
$N=\{1,\ldots,n\}\backslash B$ die Menge der
Nichtbasiskoordinaten. $A_B = (a_{j_1},\ldots,a_{j_m})\in\RR^{m,m}$
sei die zu den Basisvariablen $x_B = (x_{j_i})_i\in\RR^m$ gehörende
Teilmatrix, sowie $A_N$ analog die zu den Nichtbasisvariablen
gehörende Teilmatrix. Dann gilt $x_N=0$ und damit $A_Bx_b = b$.
Der Zielfunktionswert in $x$ ist deshalb
\begin{equation}
  c^Tx = c_B^Tx_B = c_BA_B^{-1}b.
\end{equation}
Wir betrachten nun eine beliebigen anderen Punkt $u\in M$, also $u\ge
0$ und $Au=A_Bu_B + A_Nu_N = b$. Dann ist
\begin{equation}
  \label{eq:simplexub}
  u_b = A_B^{-1}(b - A_Nu_N) = x_B - A_B^{-1}A_Nu_N
\end{equation}
und damit
\begin{equation}
  c^Tu = c_B^T(x_B - A_B^{-1}A_Nu_N) + c_N^Tu_N = c^Tx +
  \underbrace{\left(c_N - A_N^T\left(A_B^{-1}\right)^Tc_B\right)^T}_{r}u_n.
\end{equation}
Da $u_N\ge 0$ ist, kann die Zielfunktion nur verkleinert werden, wenn
eine Komponente $r_s < 0$ ist. Ist dies nicht der Fall, haben wir also
unser Optimum gefunden.

Sei nun also $s$ so gewählt, dass $r_s< 0$ minimal. Dann können wir
unser Ergebnis verbessern, indem wir $u_s>0$ wählen, aber alle
anderen Elemente von $u_N$ weiterhin bei 0 belassen. Wir nehmen also
$s$ in die Basiskoordinaten $B$ auf. Damit diese eine Basis bleiben,
müssen wir nun noch sehen, welche Gleichung dafür herausfällt. Aus
\eqref{eq:simplexub} folgt, dass $u_B = x_B - u_s A_B^{-1}a_s$.  Auch
im neuen Punkt muss aber $u_B\ge 0$ gelten, daher können wir $u_N$ nur
so groß wählen, bis für die erste Basisvariable $x_t= u_s
(A_B^{-1}a_s)_t$ gilt. Dann gilt $u_t=0$, d.h.\ diese Basisvariable
geht in die Nichtbasis über.
Wir suchen also
\begin{equation}
  u_s = \min \left\{\frac{x_{j_k}}{(A_B^{-1}a_s)_k} | k\in B,
      (A_B^{-1}a_s)_k > 0\right\} =:
  \frac{x_{j_t}}{(A_B^{-1}a_s)_t}
\end{equation}
Sind dabei alle $(A_B^{-1}a_s)_k < 0$, so dass das Minimum gar nicht
existiert, können wir $u_B$ unbegrenzt groß machen. Also ist der
zulässige Bereich unbeschränkt, und wir müssen abbrechen, da die
Funktion beliebig klein werden kann und also kein Minimum hat.

Wir tauschen nun in der Basis $B$ die gefundene beschränkende
Basisvariable  $j_t$ durch $s$ aus und erhalten die
neue Basis $B'$. In $A_B$ wird dabei einfach die Spalte $t$ durch
$a_s$ ersetzt, also
\begin{equation}
  A_{B'} = A_B + (a_s - a_{j_t})e_t^T
\end{equation}
Für die Rechnungen benötigen wir aber $A_B^{-1}$, dass wir allerdings
auch mit einer Rang-1-Formel anpassen können. Die
Sherman-Morrison-Formel liefert
\begin{equation}
  \label{eq:simplexex}
  A_{B'}^{-1} = A_{B}^{-1} - \frac{A_B^{-1}a_s - e_t}{(A_B^{-1}a_s)_t}
  \left(e_t^TA_{B}^{-1}\right).
\end{equation}

\subsubsection*{Phase I}

Wie können wir nun eine zulässige erste Lösung mit zugehörigem
Basissatz und $A_B^{-1}$ finden? Die Idee ist, zunächst die Matrix
noch einmal stark zu erweitern, sodass eine gültige Ecke einfach zu
finden ist, aber nur die Ecken des ursprünglichen zulässigen Bereichs
optimal. Phase kann dann genutzt werden, um eine solche Ecke zu finden.

Dazu betrachten wir das neue Problem
{\samepage\vspace{0.2em}\begin{equation}
  \min_z (\tikzlabel{xl}0,\ldots,\,0,\tikzlabel{xr},
  \tikzlabel{yl}1,\ldots,\,1\tikzlabel{yr})^Tz
\end{equation}
\begin{tikzpicture}[overlay, remember picture]
  \draw[decorate,decoration=brace] ($(xl) + (0,1em)$) --
  node[above] {$x$} ($(xr) + (0,1em)$);
  \draw[decorate,decoration=brace] ($(yl) + (0,1em)$) --
  node[above] {$y$} ($(yr) + (0,1em)$);
\end{tikzpicture}}
mit der Variablen $z=(x,\,y)\in\RR^{n+m}$ und den Nebenbedingungen $z\ge 0$
und
\begin{equation*}
  \begin{pmatrix}
    \tilde{A} & I 
  \end{pmatrix}
  z =
  \tilde{b}
\end{equation*}
Dabei sind die $i$-ten Zeilen von $\tilde{A}$ und $\tilde{b}$
identisch mit denen von $A$ und $b$, nur werden die Vorzeichen
getauscht, falls $b_i<0$. Dies ändert die Lösungen des
Gleichungssystems nicht, aber es gilt $\tilde{b}\ge 0$. Durch die
Identität in den hinteren Spalten hat diese Matrix unabhängig vom
ursprünglichen $A$ maximalen Zeilenrang.

Sofern die ursprüngliche zulässige Menge nicht leer war, ist das
Minimum dieser Zielfunktion offenbar Null, nämlich dann, wenn alle
$y=0$ sind. Außerdem kennen wir einen zulässigen Punkt des neuen
Problems, nämlich $(0,\,\tilde{b})$, und die zugehörige Matrix
$A_B=I$.  Daher kennen wir auch $A_B^{-1}=I$, das im Folgenden dann
nur angepasst werden muss. Wir können also die Phase I des
Simplexalgorithmus unmittelbar auf diese Matrix anwenden.

Findet der Simplex-Algorithmus nun ein Minimum, dass von Null
verschieden ist, ist offenbar die ursprüngliche zulässige Menge leer
gewesen, und wir müssen abbrechen. Ist das Minimum hingegen Null und
alle $y=0$, also in der Nichtbasis, so haben wir unseren Startwert für
die Phase II gefunden. Es kann allerdings auch passieren, dass noch zu
$y$ gehörige Indizes $j_t>n$ in der Basis vorkommen. Dann müssen wir
eine echte Koordinate $s<n$ finden, die mit $j_t$ getauscht werden
kann. Dazu suchen wir ein $s$ mit $A_B^{-1}a_s)_t\neq 0$, so dass wir
wieder unsere Austauschformel \eqref{eq:simplexex} auf die Spalten $s$
und $j_t$ anwenden können. Finden wir kein solches $s$, dann kann man
zeigen,dass die ursprüngliche Matrix linear abhängige Zeilen hatte. In
diesem Fall wird die $j_t-n$-te Zeile gestrichen. In $A_B^{-1}$ müssen
entsprechend die $j_t-n$-te Spalte und $t$-te Spalte eliminiert
werden.

Quellcode~\ref{lst:simplex} zeigt eine Implementation des
Simplexverfahrens in Python.

\subsubsection{Beispiel: Polynomapproximation}
\index{Polynomapproximation}

\begin{figure}[t]
  \centering
  \includegraphics[width=\textwidth]{plots/qr_simplex_example}
  \caption{Polynomapproximation von $\exp(x)$ (durchgezogene Kurve
    links) im Bereich $[-1,1]$. Die Punkte markieren die 10
    Stützstellen. Anders als bei der Interpolation ist hier nur ein
    Polynom zweiten Grades gesucht, dass $\exp(x)$ an den gegebenen
    Stützstellen möglichst gut annähern soll. Die rot gestrichelte
    Lösung ist dabei die Lösung der kleinsten Quadrate, die grün
    gepunktete Lösung die mit minimaler maximaler Abweichung. Im
    rechten Graphen sind die absoluten Abweichungen zwischen den
    beiden Polynomnäherungen und $\exp(x)$ gezeigt. Die zweite Lösung
    erkauft sich eine niedrigere Abweichung am rechten Rand durch
    deutliche höhere Abweichung zwischen $-0,8$ und $0$. Die maximalen
    Fehler sind dadurch gleichmäßig über das Intervall verteilt.}
  \label{fig:polyapprox}
\end{figure}

Als Beispiel für den Simplexalgorithmus und gleichzeitig noch einmal
die Pseudoinversen soll das Beispiel der Polynomapproximation
dienen. Gegeben Stützstellen $x_i, y_i$, $i=1(1)m$, suchen wir ein
Polynom $p(x)=\sum_{i=0}^{n-1} c_ix^i$, so dass
\begin{equation}
  \norm{p(x_i) - y_i} = \norm{A c - b}
\end{equation}
minimal ist. Die Matrix $A$ ist dabei analog \eqref{eq:interpol} definiert:
\begin{equation}
  A=
  \begin{pmatrix}
    1 & x_1 & x_1^2 & \ldots & x_1^{n-1}\\
    \vdots & \vdots & & \vdots \\
    1 & x_m & x_m^2 & \ldots & x_m^{n-1}\\
  \end{pmatrix}
\end{equation}
und $b=(y_i)_i$. Ist $m=n$ und die $x$-Koordinaten der Stützstellen
$x_i$ paarweise verschieden, fällt die Polynomapproximation mit der
Interpolation zusammen, da der minimale Fehler $0$ offenbar durch das
eindeutige interpolierende Polynom erreicht wird.

Anders als bei der Interpolation wählt man aber im Allgemeinen $m\gg
n$, das Polynom kann also nicht alle Stützstellen passieren, und wird
im Allgemeinen keine Stützstelle exakt treffen. Dafür vermeidet die
Polynomapproximation das Rungeproblem, da die zusätzlichen Stützpunkte
verhindern, dass das Polynom zu weit von der Zielfunktion abweicht.
Das Minimum und der Algorithmus, mit dessen Hilfe dieses berechnet
werden kann, hängen von der verwendeten Norm ab. Ist $\norm{\cdot}$
die aus dem üblichen Skalarprodukt abgeleitete 2-Norm $\norm{\cdot}_2$,
so kann man das Minimum mit Hilfe der Pseudoinversen berechnen. Ist
die Norm hingegen die Maximumsnorm, berechnet sich das Ergebnis mit
Hilfe von \eqref{eq:chebyshevappr} bzw. \eqref{eq:chebyshevapprnf} und
dem Simplexalgorithmus.

Abbildung~\ref{fig:polyapprox} zeigt die Ergebnisse für die
Approximation von $\exp(x)$ in diesen Normen. Beide Näherungen
approximieren die Funktion hinreichend gut. Unterschiede zeigen sich
allerdings in den Absolutdifferenzen $\abs{p(x) - \exp(x)}$. Das in der
2-Norm approximierende Polynom minimiert den mittleren Fehler, indem
es einen kleineren Fehler im Intervall $<0$ durch eine höheren Fehler
bei 1 erkauft. Da dies nur ein Punkt ist, wiegt der verringerte Fehler
bei negativen Zahlen schwerer. Die Minimierung der maximalen
Abweichung hingegen führt dazu, dass sich der maximale Fehler sehr
gleichmäßig über das Intervall verteilt, auch wenn dafür die Fehler bei
negativen $x$ deutlich größer sind.

\afterpage{\raggedbottom
  \lstinputlisting[style=multipage,firstline=10,
  caption={Simplexalgorithmus zur Lösung des linearen Programms $\min
    c^Tx$ unter den Nebenbedingungen $Ax=b$, $x\ge 0$. Es werden keine
    besonderen Eigenschaften von $A$ vorausgesetzt, der Algorithmus
    eliminiert selbstständige redundante Zeilen.},
  label=lst:simplex]{simplex.py} \clearpage }


\section{Globale Optimierung}
\index{Optimierung>globale}

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/lj}
  \caption{Links: das Lennard-Jones-Potential mit dem typischen Radius
    $\sigma$ und der Tiefe des attraktiven Tals von
    $\epsilon$. Rechts: Energielandschaft für ein LJ-Teilchen in einer
    zweidimensionalen LJ-Flüssigkeit. Rote Punkte markieren die
    Positionen anderer Teilchen, die Graustufen repräsentieren das
    Potential. Die gelben Kreuze markieren lokale Minima. Um diese zu
    finden, wurde zunächst das Potential an $100\times 100$ Punkten
    ausgewertet, und von allen Punkten mit einem Potential kleiner als
    1000 aus eine lokale Minimierung gestartet.}
  \label{fig:ljminima}
\end{figure}

Die Ausgleichsrechnung, also $\min \norm{Ax-b}_2$, und die linearen
Programme sind zwei Spezialfälle von Optimierung, bei denen ein
gefundenes Optimum stets auch ein globales Minimum ist. Das hängt
damit zusammen, dass die Zielfunktionen im Allgemeinen nur ein
(Ausgleichsrechnung) oder gar kein lokales Minimum (lineare
Programmierung) haben. Im Gegensatz dazu finden das Newtonverfahren
oder das Verfahren des steilsten Abstiegs stets nur lokale Minima,
also Punkte mit verschwindendem Gradienten, dafür aber bei fast
beliebiger Zielfunktion.

In der Praxis, und besonders in physikalischen Anwendungen, sind die
Zielfunktionen meist stark nichtlinear und besitzen zahlreiche lokale
Minima. Suchen wir etwa Grundzustände eines komplexen Systems, müssen
wir Minima der Energiefunktion finden, die meist so viele lokale
Nebenminima aufweist, dass es auch für einen Computer unmöglich ist,
alle zu untersuchen.

Als Beispiel betrachten wir ein in der statistischen Physik sehr
beliebtes Modell, das Lennard-Jonesium (LJ). Dabei handelt es sich um
Teilchen, die gemäß einem Potential
\begin{equation}
  \phi_{\text{LJ}}(r) = 4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12} -
    \left(\frac{\sigma}{r}\right)^{6}\right]
\end{equation}
wechselwirken (vgl. Abbildung~\ref{fig:ljminima} links). Das Potential
hat ein Minimum der Tiefe $-\epsilon$ bei $\sqrt[6]{2}\sigma$ und wird
unterhalb $\sigma$ rasch sehr groß. $\phi_{\text{LJ}}$ ist ein
einfaches Modell für Edelgasatome mit einem Durchmesser von etwa
$\sigma$, die sich auf längere Abstände dispersiv ($\sim r^{-6}$)
anziehen.

In einer solchen zweidimensionalen Lennard-Jones-Flüssigkeit aus 100
Teilchen in einer $10\sigma\times 10\sigma$-Box wählen wir nun ein
Teilchen aus und halten die Koordinaten aller anderen Teilchen
fest. Abbildung~\ref{fig:ljminima} rechts zeigt die potentielle
Energie in Abhängigkeit von der Position des ausgewählten
Teilchens. Dies ist aber nichts anderes als ein zweidimensionaler
Schnitt durch die potentielle Energie als Funktion \emph{aller}
Teilchenkoordinaten. Um die lokalen Minima zu bestimmen, wurde
zunächst auf einem $100\times 100$-Raster das Potential ausgewertet,
und von allen hinreichend niedrigen Punkten aus eine lokale
Minimierung gestartet. Da bereits dieser Schnitt über 20 lokale Minima
aufweist, kann man sich vorstellen, wie viele lokale Minima die
Energie als Funktion aller 200 Teilchenkoordinaten aufweist.  Wegen
des ``Curse of dimension'' ist es in diesem Fall nicht mehr möglich,
eine Rasterung vorzunehmen. Es ist also aussichtslos, sämtliche lokale
Minima mit Hilfe eines lokalen Minimierungsverfahrens zu suchen.

Im Folgenden lernen wir zwei heuristische Methoden kennen, die auch
unter solchen Bedingungen meist akzeptable Näherungen für das globale
Minimum finden.

\subsection{\keyword{Simulated annealing}}

Simulated Annealing (Simulierte Abkühlung) basiert auf der
Beobachtung, dass durch gesteuerte, langsame Abkühlung das Wachstum
besonders gleichmäßiger Kristalle gefördert wird. Dies macht man sich
zum Beispiel bei der Härtung von Stahl zu Nutze. Letztendlich ist
Kristallisation aber nichts anderes als ein Optimierungsprozess, da
der Einkristall die energetisch minimale Struktur ist.

Um dies auf ein Optimierungsproblem zu übertragen, legt man die
Boltzmann-Statistik der statistischen Physik zugrunde, die besagt,
dass bei Temperatur $T$ alle Zustände gemäß
\begin{equation}
  P_T(x)\sim e^{-\beta E(x)}
\end{equation}
verteilt sind, wobei $E$ die potentielle Energie des Systems im
Zustand $x$ ist und $1/\beta=k_BT$ mit der Boltzmannkonstanten $k_B$.

Sei nun $x_0$ das globale Minimum der Energiefunktion $E$. Dann ist
für alle $x$
\begin{equation}
  P_T(x)\sim e^{-\beta E(x)} = e^{-\beta E(x_o)}e^{\beta [E(x_0) - E(x)]}
  \sim e^{-\beta [E(x) - E(x_0)]} \le 1,
\end{equation}
da $E(x) - E(x_0)\ge 0$. Bei niedrigen Temperaturen ist $\beta$ sehr
groß, so dass alle Zustände mit einer größeren Energie als $E(x_0)$
rasch sehr unwahrscheinlich werden. Durch Abkühlen wird es also
tatsächlich sehr wahrscheinlich, das globale Minimum zu finden. Um
damit ein Optimierungsproblem $\min_{x} E(x)$ global zu lösen, fassen
wir die Elemente der zulässigen Menge als Zustände auf, und die zu
minimierende Funktion $E$ als Energie dieser Zustände.

Im Prinzip könnte man nun mit Hilfe der Verwerfungsmethode versuchen,
$P_T$-verteilte zufällige Zustände zu erzeugen. Wie wir im
Eingangsbeispiel gesehen haben, schwankt die Energielandschaft
allerdings oft stark, so dass die meisten Punkte extrem kleine
Akzeptanzraten hätten. Dies sorgt für astronomisch hohe
Verwerfungsraten, so dass es mit dieser Methode unmöglich ist, die
Boltzmann-Verteilung abzutasten. In der statistischen
Physik benutzt man als Ausweg das \emph{Monte-Carlo-Sampling} von
N. Metropolis.

\index{Monte-Carlo-Sampling}\index{Metropolis-Methode} Ziel dieser
Methode ist es, nacheinander Punkte $x_i$ zu erzeugen, die
$P_T$-verteilt sind, so dass die Folge $x_i$ unsere Verteilung
repräsentiert. Nehmen wir nun an, dass unser aktueller Punkt bereits
$P_T$-verteilt gezogen wurde, müssen wir also den nächsten Punkt so
wählen, dass wir die Verteilung nicht ändern. Dazu wählen wir
zufällige Übergänge $x_i\to x_{i+1}$ so, dass für zwei beliebige
Punkte $x$ und $y$ die Übergänge $x\to y$ und $y\to x$ mit gleicher
Wahrscheinlichkeit ausgewählt werden. Eine Möglichkeit ist der
klassische \emph{Metropolis-Schritt}
\begin{equation}
  y = x_i + d,\quad\text{mit}\; d\;\text{gleichverteilt auf einer
    Kugel vom Radius}\; r.
\end{equation}
$r$ ist dabei in gewisser Weise die maximale Schrittweite dieses
Algorithmus. Dies erzeugt eine Gleichverteilung der Punkte; um die
korrekte Verteilung zu erzeugen, setzen wir $x_{i+1}=y$ mit
Wahrscheinlichkeit proportional zu
\begin{equation}
  p(y) = \min(e^{-\beta [E(y) - E(x_i)]}, 1) = \min(P(y)/P(x_i), 1).
\end{equation}
Ist also $E(y) \le E(x_i)$, akzeptieren wir den Schritt immer, ist
$E(y) > E(x_i)$, dann mit Wahrscheinlichkeit $e^{-\beta [E(y) -
  E(x_i)]}$. Die Verwerfungsmethode liefert uns einen einfachen
Algorithmus, diese Verteilung zu erzeugen --- wir ziehen eine
Standardzufallszahl $u$ und setzen $x_{i+1}=y$, falls $u<p(y)$ und
$x_{i+1}=x_i$ sonst.

\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{plots/simulated_annealing}
  \caption{Simulated Annealing eines zweidimensionalen
    Salzschmelzemodells. Links sind die gefundenen Grundzustände für
    verschiedene Abkühlraten $\epsilon$ gezeigt, rechts die
    dazugehörigen potentiellen Energien als Funktion der Zeit (schwarz
    gepunktet $\epsilon=0,1$, grün durchgezogen $\epsilon=0,05$, blau
    gestrichelt $\epsilon=0,01$ und magentafarbene Strichpunkte
    $\epsilon=0,005$). Der eigentlich Grundzustand wäre ein
    NaCl-artiges Gitter, das aber nicht gefunden wird, auch wenn die
    Simulationen immer größere Gitterflächen zeigen. Die Unterschiede
    in den finalen Energien sind sehr klein, was nochmals zeigt,
    dass auch dieses Problem sehr viele Nebenminima aufweist.}
  \label{fig:simanneal}
\end{figure}

Beim Simulated Annealing werden die Punkte mit Hilfe der
Metropolis-Methode $P_T$-verteilt erzeugt, und gleichzeitig die
Temperatur langsam abgesenkt. Dadurch sollte die Verteilung langsam
gegen $P_0$ konvergieren, in der nur noch das oder die globalen Minima
eine nichtverschwindende Wahrscheinlichkeit haben. Dementsprechend
konvergiert die Folge $x_i$ gegen ein globales Minimum. Problematisch
kann sein, dass die korrekte Verteilung der $x_i$ natürlich nur
sichergestellt werden kann, wenn auch hinreichend viele $x_i$
generiert werden. Das bedeutet, dass die Temperatur langsam abgesenkt
werden sollte, wobei ``langsam'' vom Problem abhängt.

Die funktionale Form der Temperaturkurve ist frei
wählbar. Typischerweise wählt man eine exponentielle Form
\begin{equation}
  T_{i+1} = (1-\delta)T
\end{equation}
mit kleinem $\delta > 0$ (etwa $10^{-6}$). Dann wird natürlich $T=0$
nie erreicht, allerdings sind kleine $T$ meist ausreichend. Alternativ
kann man eine lineare Form wählen
\begin{equation}
  T_{i+1} = T_i - \delta,\quad\text{solange}\; T_i>\delta.
\end{equation}

Simulated Annealing funktioniert nicht nur mit dem
Metropolis-Schritt, sondern auch mit allen anderen möglichen
Monte-Carlo-Schritten, wie sie in der Vorlesung
"`Simulationsmethoden"' behandelt werden. Auch Molekulardynamik kann
genutzt werden, um Zustände im kanonischen Ensemble, also bei
konstanter Temperatur, zu erzeugen.

\subsubsection{Beispiel: zweidimensionale Salzschmelze}
Als Beispiel zeigt Abbildung~\ref{fig:simanneal} Daten zur
Kristallisation eines einfachen NaCl-Modells. Hier wechselwirken die
Teilchen mit einem Potential
\begin{equation}
  \phi_{ij} = q_iq_j\frac{4\sigma\epsilon}{r}  +
  \begin{cases}
    4\epsilon\left(\frac{\sigma}{\norm{x_i-x_j}}^{12} -
      \frac{\sigma}{\norm{x_i-x_j}}^{6}\right) + \epsilon
    & \text{für}\; \norm{x_i-x_j} < \sqrt[6]{2}\sigma\\
    0 & \text{sonst}.
  \end{cases}
\end{equation}
Die Hälfte der Teilchen haben dabei Ladung $q_i=1$, die andere Hälfte
$q_i=-1$. Der erste Teil des Potentials ist das Coulombpotential
aufgrund der elektrostatischen Anziehung bzw. Abstoßung, der zweite
besteht aus dem repulsiven Teil des LJ-Potentials und simuliert die
Volumenausschlußwechselwirkung. Die Teilchen arrangieren sich aufgrund
des Volumenausschlusses recht schnell in einem Gitter, es kostet
allerdings sehr viel Energie, dann Plätze zu tauschen. Daher gibt es
zahlreiche Nebenminima.

Im Beispiel wurde statt des Metropolisalgorithmus eine
Molekulardynamiksimulation mit Hilfe von ESPResSo~\cite{espresso}
benutzt. Auch hier kann die Temperatur gesteuert werden. Links zeigt
Abbildung~\ref{fig:simanneal} die gefundenen Annäherungen für
Grundzustände, rechts sind deren Energien gezeigt. Trotz der recht
langen Simulationszeiten sind die gefundenen Grundzustände noch nicht
optimal --- das wäre ein quadratisches Gitter mit alternierender
Besetzung. Eine Monte-Carlo-Simulation könnte hier übrigens deutlich
effizienter sein, allerdings nur, wenn auch sogenannte
Identitätstauschschritte benutzt werden, bei denen zwei beliebige
Teilchen ihre Ladungen austauschen und so die Energiebarrieren
umgehen.

\subsection{Genetische Algorithmen}

Für die Untersuchung noch komplexerer Kristallstrukturen, etwa mit -3-
und 5-wertigen Ionen, ist der obige Ansatz immer noch nicht effizient
genug, da er dazu neigt, in lokalen Minima stecken zu bleiben. Besser
wäre es, Elementarzellen des Kristalls durchzuprobieren, und nach
derjenigen mit minimaler Energie zu suchen. Leider ist auch diese
Optimierung noch sehr umfangreich, und die Menge der möglichen
Kristallstrukturen lässt sich nicht einfach als Vektorraum beschreiben.

In solchen Fällen sind genetische Algorithmen ein biologisch
inspirierter Ansatz, um trotzdem eine globale Minimierung durchführen
zu können. Dabei ist das \emph{Genom} nicht mehr ein DNS-Molekül,
sondern eine Zeichenkette aus reellen Zahlen, Bits oder ähnlichem, die
jeweils ein Element der ursprünglichen zulässigen Menge codieren (also
zum Beispiel eine mögliche Elementarzelle). Wir erzeugen zu Beginn der
genetischen Algorithmus zufällig eine große Menge von Individuen (etwa
einige 1000), indem wir zufällige entsprechende Zeichenketten
generieren.

Wie bei der natürlichen Evolution sollen sich im Verlauf die
"`fittesten"' Individuen sich vermehren und ungeeignetere aussterben.
Dazu definieren wir eine \emph{Fitnessfunktion}, die die Güte eines
Genoms bzw.\ des dadurch codierten Individuums misst. Im Falle der
Kristallgitteroptimierung wäre dies zum Beispiel die Energie eines
Gitters, dass aus der Elementarzelle entsteht, die vom Genom codiert
wird.

Wie werden nun Individuen vermehrt? Einfach, indem zufällige, kleine
Änderungen am Genom des sich vermehrenden Individuums vorgenommen
werden und dieses neue Genom in den Pool der vorhanden Individuen
aufgenommen wird. Dabei werden fittere Individuen häufiger
vermehrt. Das kann man auf verschiedene Weisen erreichen, zum
Beispiel, indem stets eine kleine Anzahl von Individuen zufällig
ausgewählt wird, und von diesen das fitteste vermehrt.  Dabei gibt es
zwei Typen von Änderungen am Genom: \emph{Mutationen}, also kleine
zufällige Änderungen im Genom, und \emph{Kreuzungen} zweier guter
Individuen. Dabei werden zufällig Abschnitte der beiden ausgewählten
Individuen zu einem neuen Genom kombiniert.

Um die Menge der Genome konstant zu halten, können bei diesem Prozess
auch Genome entfernt werden. Werden etwa die fittesten Individuen
einer zufällig gewählten Gruppe zur Vermehrung herangezogen, kann
gleichzeitig das am wenigste fitte Element dieser Gruppe gelöscht
werden.

L. Filion und M. Dijkstra~\cite{filion09a} haben einen solchen
Algorithmus benutzt, um die möglichen Kristallstrukturen in binären
kolloidalen Kristallen mit verschiedenen Größen und Ladungen
vorherzusagen. Dabei werden die Elementarzellen durch eine feste
Anzahl von Vektoren dargestellt. Hat die Elementarzelle $n$ Elemente,
stellen $n-1$ Vektoren $B_i$ die Relativpositionen der Elemente in der
Zelle dar, und 3 Vektoren $L_i$ legen die Relativpositionen der
räumlichen Kopien der Elementarzelle fest. In den meisten zufällig so
erzeugten Gittern überlappen Teilchen, so dass auch in diesem Fall ein
weiches Lennard-Jones-Potential die Volumenausschlusswechselwirkung
modelliert. Dadurch werden undefinierte Energiewerte vermeiden.

Zur Vermehrung werden zufällig zwei Individuen gewählt, deren Vektoren
$B_i$ zufällig um maximal $\pm 10\%$ gestreckt werden. Dann wird
zufällig $B_1,\ldots,\,B_k$ vom ersten Individuum mit
$B_{k+1},\ldots,\,B_n$ vom zweiten kombiniert, und analog die
$L_i$. Zusätzlich wird sicher gestellt, dass die $L_i$ linear
unabhängig sind, also tatsächlich ein Gitter aufspannen. Um die
Chancen des neuen Genoms zu erhöhen, wird für dieses die Energie lokal
minimiert, indem die Darstellung geeignet verdreht wird. Es ist durchaus
üblich, physikalisches Wissen in genetischen Algorithmen zu benutzen,
um bessere Genome zu erzeugen.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "padc.tex"
%%% TeX-PDF-mode: t
%%% End: 
